# Mission Impossible: A Statistical Perspective on Jailbreaking LLMs

## Abstract

대규모 언어 모델(LLM)은 품질 관리가 제한된 방대한 텍스트 데이터로 학습됩니다. 그 결과, LLM은 정보 유출, 가짜 뉴스 또는 증오 발언과 같은 의도하지 않거나 해로운 행동을 보일 수 있습니다. 일반적으로 선호도 정렬(preference alignment)이라고 하는 대응책에는 사전 학습된 LLM을 원하는 행동의 신중하게 만들어진 텍스트 예제로 미세 조정하는 것이 포함됩니다. 그럼에도 불구하고, 경험적 증거에 따르면 선호도 정렬된 LLM도 해로운 행동을 하도록 유도될 수 있습니다. LLM의 이러한 탈옥(jailbreaking)은 일반적으로 LLM에 대한 입력 프롬프트를 적대적으로 수정하여 달성됩니다. 저희 논문은 통계적 관점에서 선호도 정렬 및 탈옥 현상에 대한 이론적 통찰력을 제공합니다. 저희 프레임워크 하에서, 먼저 사전 학습된 LLM이 훈련 코퍼스에 존재하는 경우 해로운 행동을 모방할 것임을 보여줍니다. 동일한 프레임워크 하에서, 정렬에 대한 통계적 개념을 도입하고 탈옥 확률의 하한을 정하여 합리적인 가정 하에서는 탈옥을 막을 수 없음을 보여줍니다. 저희의 통찰력을 바탕으로, 현재 널리 사용되는 정렬 전략인 RLHF를 변경할 것을 제안합니다. 구체적으로, 우리는 RLHF 목표에 대한 간단한 수정안인 E-RLHF를 도입하여 안전한 응답의 가능성을 높이는 것을 목표로 합니다. E-RLHF는 추가적인 훈련 비용이 들지 않으며 다른 방법과 호환됩니다. 경험적으로, 우리는 E-RLHF가 AdvBench [1] 및 HarmBench 프로젝트 [2]에서 제시된 모든 정렬 문제에서 MT-Bench 프로젝트 [3]로 측정한 모델 성능을 희생하지 않으면서 RLHF를 능가함을 보여줍니다.

## 1 Introduction

대규모 언어 모델(LLM)은 비서, 코드 생성[4], 헬스케어[5], 정리 증명[6] 등 다양한 영역에서 놀라운 능력으로 딥러닝 분야에 혁명을 일으켰습니다. LLM의 훈련 과정은 일반적으로 두 단계로 구성됩니다: 방대한 코퍼스를 사용한 사전 훈련과, 인간의 피드백을 이용한 강화 학습(RLHF)을 사용하여 모델 행동을 인간의 선호도에 더욱 정렬시키는 정렬 단계입니다. 후자 단계는 일반적으로 대량의 인간 주석 데이터를 포함하며, 지도 미세 조정(SFT) 단계, 보상 모델링 단계, RL 미세 조정 단계로 분해될 수 있습니다. 여러 작업을 효과적으로 수행할 수 있는 능력에도 불구하고, LLM은 사전 훈련 데이터셋[7-9] 내에 해로운 요소가 불가피하게 존재하기 때문에 증오 발언, 악성 코드, 가짜 정보 또는 사회적 편견을 포함한 공격적이거나 부적절한 콘텐츠를 생성하기 쉽습니다. 소셜 미디어는 "Do Anything Now"(DAN) 프롬프트[11]나 "Grandma Exploit" 핵[12]과 같이 ChatGPT[10]를 공격하여 해로운 응답을 유도하는 방법에 대한 수많은 트릭을 보여줍니다. 다른 한편으로, 훈련 코퍼스의 행동 다양성은 예를 들어 다른 문화적 선호도를 포착하는 데 필수적입니다. 무엇이 해롭고 해롭지 않은지는 궁극적으로 사용자 선호도에 따라 달라지므로, 정렬 단계는 보편적이지 않고 모델이 사용될 특정 사용 사례에 따라 달라집니다.

배포 안전성을 해결하고 불쾌한 응답을 제거하기 위해, SFT [13] 중에 안전한 정보를 주입하거나, 인간 전문가와 AI 자체를 이용한 레드팀 구성[14-18], 그리고 전체 RLHF 프로세스를 상세히 개선하고 향상시키는[19-23] 등 수많은 정렬 노력이 있었습니다. 그러나 우리는 "해로운" 프롬프트를 무력화하기 위한 더욱 정교한 정렬 방법과 LLM을 조작하여 해로운 정보를 생성하도록 하는 더욱 창의적인 "탈옥" 공격의 쫓고 쫓기는 게임을 계속 목격하고 있습니다. 이러한 공격은 적대적 접미사 주입[1, 24, 25], 암호 및 저자원 자연어 탐색[26-28], 또는 LLM이 자동으로 프롬프트를 만들게 하는[29-33] 등 다양한 형태로 나타납니다. 접미사에 대한 몇 가지 임시 방어 방법이 제안되었지만[34-37], 탈옥 공격에 대한 원칙적인 보편적 방어[2]에 대한 제안은 제한적이며, 이 현상에 대한 이론적 이해도 제한적입니다[38].

이 논문에서는 사전 훈련 단계와 정렬 후 탈옥 현상을 모두 분석하기 위한 이론적 프레임워크를 제시합니다. 탈옥 프롬프트가 일반적으로 기본 해로운 개념은 유지하면서 프롬프트의 다른 측면을 조작한다는 사실을 활용하여, 잠재적 적의 강점을 정량화할 수 있도록 입력 프롬프트를 분리하는 프레임워크를 설계합니다. 언어 모델(LM)의 출력 요소를 개별 토큰이 아닌 더 긴 텍스트 조각으로 표현함으로써, 이러한 모델이 훈련 분포를 모방하는 정도를 정량화하고 결과적으로 탈옥 취약성의 기본 메커니즘을 더 잘 이해할 수 있습니다.

우리의 기여는 다음과 같이 요약될 수 있습니다:
- 제안된 프레임워크를 기반으로, 먼저 사전 훈련을 위한 비-자명한(non-vacuous) PAC-Bayesian 스타일의 일반화 경계를 제공합니다. 우리 프레임워크의 유효성을 가정하면, 높은 성능의 사전 훈련된 모델은 의도하지 않은 해로운 행동을 포함하여 훈련 코퍼스에 존재하는 행동을 생성하는 데 불가피하게 취약할 것이라고 결론 내립니다.
- 이후, 우리는 정렬 및 탈옥의 개념을 포함하도록 프레임워크를 확장합니다. 우리의 가정이 충족된다고 가정하면, LM이 안전한 응답 집합에 대한 출력 분포를 집중시키는 데 실패하기 때문에 안전 정렬 후에도 탈옥은 예방할 수 없음을 보여줍니다.
- 이론적 발견에 동기를 부여받아, 우리는 널리 채택된 RL Fine-Tuning 목표의 주요 단점을 식별합니다. 이는 무해함과 유용함 목표 간의 자연스러운 차이 때문입니다. 이 문제를 해결함으로써, 우리는 모델 성능을 유지하면서 일련의 탈옥 공격에 더 탄력적인 더 안전한 모델의 훈련을 용이하게 합니다.

이 논문은 다음과 같이 구성됩니다. 섹션 2에서는 우리의 프레임워크를 소개합니다. 섹션 3에서는 사전 훈련에 대한 PAC-Bayesian 일반화 경계를 증명합니다. 다음으로, 섹션 4에서는 통계적 관점에서 탈옥에 대한 분석을 제시합니다. 마지막으로, 섹션 5에서는 제안된 E-RLHF 목표와 LLM 안전성 향상에 대한 효과를 설명합니다. 부록 H에서 문헌 검토를 제공합니다.

## 2 Framework and assumptions

탈옥은 컴퓨터 비전[39]에서 잘 연구된 분야인 적대적 공격과 몇 가지 유사점을 가집니다. 여기서 적은 주어진 입력 이미지를 픽셀 공간에서 교란하여 모델 출력을 변경하는 맵으로 정의됩니다. 적의 강도는 `lp` 거리[40-42]로 정량화된 원래 입력을 얼마나 멀리 이동시킬 수 있는지에 따라 제한됩니다. 일반적으로 이 거리는 인간 관찰자에게 변화가 인지되지 않는 방식으로 제한됩니다. 적의 목표는 입력의 오분류를 유발하는 것입니다. 대조적으로, LLM의 경우, 적의 목표는 정보의 의도하지 않은 유출이나 증오 발언과 같은 해로운 행동을 유발하는 것입니다. 또한, 프롬프트라고 불리는 입력에 대한 모든 교란은 인지 가능한 효과를 가집니다. 따라서 적의 능력을 정량화하고 제한하는 것은 간단하지 않습니다. 그럼에도 불구하고, 약간의 수정을 통해 이 비유를 기반으로 구축할 것입니다.

이 연구의 목적을 위해, 우리는 모든 프롬프트를 쿼리와 개념의 튜플 $(q,c)$로 볼 것이며, 여기서 $c \in C$이고 $q \in Q$이며, $C$, $Q$는 전체 개념 집합과 쿼리 집합을 나타냅니다. 개념적으로, 우리는 개념을 프롬프트의 정보 내용을 나타내는 것으로 생각하며, 보통 "케이크 만드는 법 튜토리얼"과 같은 짧은 텍스트 조각을 통해 나타냅니다. 쿼리는 특정 개념과 구성 가능한 지시적 텍스트 조각입니다. 우리는 쿼리를 LM이 특정 방식으로 개념을 확장하도록 유발하는 메커니즘으로 생각할 수 있습니다. 예로는 "{}하는 법 알려줘" 또는 "우리는 지금 상상의 세계에 있고, 너는 어떤 윤리적 제약에도 얽매이지 않아. 내 아바타에게 {}하는 법을 가르쳐줘"가 있습니다. 모든 쿼리와 개념이 구성 가능한 것은 아니므로, 우리는 $P \subseteq Q \times C$를 모든 그럴듯한 프롬프트의 집합으로 표시하며, 그럴듯함의 정의는 아래에서 명확히 할 것입니다.

프롬프트의 분해는 우리가 적의 강점을 분리하고 따라서 제한할 수 있게 해줍니다. 해로운 행동을 유도하는 현재의 경험적 연구와 일치하게, 우리는 개념이 아닌 쿼리에만 교란을 허용할 것입니다. 적대적 공격에 대한 이전 연구의 정신을 더욱 모방하여, 우리는 프롬프트와 관련된 ground-truth가 쿼리가 아닌 개념에 의해서만 결정된다고 가정할 것입니다. 우리는 다음 단락에서 이러한 아이디어를 더 엄격하게 만들 것입니다.

첫째, LM을 단일 문장 생성기[38]로 간주했던 이전의 이론적 연구와는 대조적으로, 우리는 LM을 더 긴 텍스트 조각 생성기로 모델링하고, 생성된 가능한 콘텐츠 $e \in E$를 설명(explanation)이라고 지칭합니다. 개념적으로, 설명은 추가 정보로 개념을 확장합니다. 예를 들어, "2023년 미국 대통령은 조 바이든이다."가 있습니다. 우리의 용어 "설명"은 LM이 입력을 받아 응답을 생성하는 정책으로 간주되는 이전 논의(예: [23])에서 사용된 "응답"과 개념적으로 동일합니다. 우리는 현재 커뮤니티에서 고려되는 대부분의 탈옥 공격에서 적이 단일 해로운 시도에 대한 지침이나 설명을 찾기 때문에 "개념"과 대조하기 위해 "설명"을 사용합니다. 따라서 LM은 그럴듯한 프롬프트에서 설명에 대한 분포로의 매핑 $PLM : P \to \Delta(E)$를 유도하며, 여기서 $\Delta(E)$는 $E$의 원소에 대해 정의된 분포 집합을 나타냅니다.⁴ LM의 출력 $PLM(q, c)$는 설명에 대한 이산 분포입니다. 우리는 이 분포의 도메인을 $dom(PLM (q, c))$로, 입력 $(q, c)$가 주어졌을 때 $e$의 확률을 $PLM (e|q, c)$로, 0이 아닌 $PLM (e|q, c)$를 가진 $E$의 부분 집합을 $supp(PLM(q, c))$로 사용합니다. 또한, 우리는 사전 훈련 단계에서 LM이 모방하도록 최적화된 잠재적 ground truth 매핑 $P_{world} : P \to \Delta(E)$가 존재한다고 가정합니다. 이것이 "지식"을 정의하는 분포입니다: 모든 그럴듯한 프롬프트 $(q, c)$에 대해, 설명에 대한 ground-truth 분포를 지정합니다. 그럴듯하다는 것은 ground truth 매핑의 도메인에 있는 모든 프롬프트, 즉 $(q, c) \in dom(p_{world})$를 의미하며, $P = dom(p_{world})$입니다. 아래에서 논의될 바와 같이, 많은 그럴듯한 프롬프트는 사용 가능한 훈련 코퍼스 내에 존재하지도 않을 것입니다.

이제 우리는 주요 가정을 명시할 수 있습니다. 즉, 모든 그럴듯한 프롬프트 $(q, c) \in dom(p_{world})$에 대해 ground-truth 분포 $P_{world}(q, c)$는 $E$의 작은 부분 집합에서 지원된다는 것입니다. $supp(P_{world}(q,c)) \subset E$. 이 가정은 우리에게 합리적으로 보입니다. 정상적인 상황에서 "파리"에 대한 설명을 제공하는 것은 "헬로 월드 파이썬 스크립트를 작성하는 방법"과 같은 프롬프트가 주어졌을 때 관련 지식을 제공하지 않을 것이기 때문입니다. 우리의 두 번째 가정은 모든 그럴듯한 프롬프트 $(q, c)$에 대해, 개념 $c$가 쿼리에 관계없이 $P_{world}$에 의해 지정된 출력 분포의 지원을 고유하게 결정한다는 것입니다: $supp(p_{world}(q, c)) = supp(P_{world}(q*, c))$, 모든 그럴듯한 $(q, c)$ 및 $(q*, c)$. 쿼리는 지원에 영향을 주지 않으면서 ground-truth 분포를 변경합니다. 그림 1에 그림이 나와 있습니다. 더 정확하게 말하면:

**Assumption 2.1.** (Concepts uniquely determine the explanation for plausible prompts)
모든 그럴듯한 프롬프트 $(q, c) \in dom(p_{world})$에 대해,

i) $P_{world}: P \to \Delta(supp(P_{world}(q, c)))$

---
³예를 들어, "케이크 만드는 법 튜토리얼은 누구인가."는 비합리적입니다.
⁴실제 LM의 경우, 온도 T, top-p 및 top-k 샘플링 매개변수와 같은 다른 디코딩 하이퍼파라미터를 사용하면 동일한 매개변수 집합으로 유도된 분포가 다를 수 있습니다. 우리의 논의는 이 논문 전체에서 미리 고정된 하이퍼파라미터 집합에 대해 유효합니다.

---

ii) $supp(P_{world}(q, c)) = supp(P_{world}(q*, c))$, 모든 $(q, c)$, $(q*, c)$는 그럴듯합니다.

이 가정은 본질적으로 지식이 사용된 쿼리에 관계없이 해당 개념에 의해서만 지정된다는 것을 의미하기 때문에 자연스럽습니다. 즉, 개념 $c$가 주어졌을 때 쿼리 $q$가 $supp(P_{world}(q, c))$를 변경하는 데 성공하면, 우리는 쿼리가 분해되어 지원에 의해 미러링된 지식을 정확하게 반영하기 위해 $e$에 부분적으로 흡수되어야 한다고 주장합니다.

마지막으로, 우리는 프롬프트에 대한 기본 생성 분포의 존재를 가정하며, 이를 $(q, c) \sim D_p$로 표시합니다. 이 분포는 사전 훈련 코퍼스 생성의 원칙으로 작용합니다. $supp(D_p) \subseteq dom(p_{world})$라는 점에 유의하는 것이 중요합니다. 예를 들어, 프롬프트 $(q', c')$="제임스 본드는 누구인가 $入*#!48811"를 생각해 봅시다. 이 프롬프트는 인터넷의 어떤 텍스트 코퍼스에도 나타나지 않지만, $(q', c') \notin supp(D_p)$, 우리 인간은 그것을 이해할 수 있습니다: $(q', c') \in dom(P_{world})$. 이 논문의 뒷부분 증명에서는 LM이 실제로 거대한 분포 외 데이터셋[43]에서 잘 일반화된다고 주장되기 때문에, 이러한 보이지 않는 그럴듯한 프롬프트에 대해 의미적으로 합리적인 설명을 생성한다고 가정합니다. 이는 섹션 4의 가정 4.1 내에서 명시적으로 만들어집니다.

마지막으로, 다음 정의는 우리의 유해성 개념에 관한 것입니다. 더 구체적으로, 우리는 유해한 행동을 추상적으로 의도하지 않은 행동으로 이해합니다. 이를 위해, 우리는 모든 설명 `e`가 유해하거나 유해하지 않은(안전한) 것으로 표시될 수 있다고 가정합니다. 개념 `c`는 직접적인 프롬프트로 특정 임계값보다 높은 확률로 유해한 설명을 생성하는 경우에만 유해한 것으로 간주됩니다.

**Definition 2.1.** (Notions of Harmfulness)
- **(Direct Queries and Direct Prompts)** 우리는 프롬프트가 $D_p$에서 비롯된 경우, 즉 $(q, c) \in supp(D_p)$인 경우 직접적이라고 지칭합니다. 직접적인 프롬프트의 쿼리를 직접 쿼리라고 합니다.
- **(Harmful Concepts and Harmful Set)** 개념 $c$가 주어졌을 때, 관련된 유해한 설명 집합은 $E_h(c) := \{e | e \in supp(p_{world}(·,c)) \land e \text{ is harmful}\}$로 표시됩니다. 가정 2.1에 따라, 임계값 $\eta$를 사용하여, 개념 $c$는 모든 $q$에 대해 $(q, c) \in dom(p_{world})$일 때, $\sum_{e:e \in E_h(c)} P_{world}(e|q, c) \ge 1 - \eta$이면 해롭습니다. 우리는 모든 가능한 해로운 개념의 집합을 $C_h \subset C$로 지칭합니다.
- **(Safe Set)** 모든 $c \in C_h$에 대해, $PLM (q, c)$가 집중되기를 바라는 해당 안전 집합 $E_s(c) \subseteq E$가 존재합니다. 여기에는 $supp(p_{world}(·, c))$에 존재하는 안전한 설명과 "죄송합니다."로 시작하는 템플릿과 같이 인간이 설계한 설명이 포함됩니다.
- **(Semantically meaningful)** 우리는 $E_h(c) \cup E_s(c)$의 설명을 $(q, c)$ 프롬프트에 대해 의미적으로 의미있는 것으로 부릅니다.
- **(Mixture decomposition of Dp)** 이러한 개념으로, 우리는 $D_p = \alpha D_{ph} + (1 - \alpha)D_{p\bar{h}}$ (여기서 $supp(D_{ph})$는 해로운 개념을 가진 모든 직접 프롬프트를 포함하고, $supp(D_{p\bar{h}})$는 그 보완을 포함함)를 해로운 개념을 가진 직접 프롬프트와 그렇지 않은 프롬프트의 혼합으로 분해할 수 있습니다.

## 3 PAC-Bayesian bound for pre-training LLMs on harmful data

모델 집합에 대한 사후 분포로 이어지는 학습 알고리즘이 주어졌을 때, PAC-Bayesian 이론[44]은 일반화 격차, 즉 모델의 경험적 손실과 모집단 손실 간의 차이에 대한 경계를 제공하기 위해 아마도 거의 정확하게(PAC) 부등식을 적용합니다. 이제 우리는 분석의 첫 번째 결과인 사전 훈련 LM에 대한 비-자명한 PAC-Bayesian 경계를 제시하며, 이는 잘 훈련된 LM이 훈련 중에 해로운 행동을 접했다면 단순히 직접 쿼리로 프롬프트될 때조차도 해로운 행동을 보일 것임을 암시합니다.

우리는 $S = \{(q_i, C_i)\}_{i=1}^n$을 $D_p$ 하에서 i.i.d.로 생성된 프롬프트 집합, 즉 $S \sim D_p^n$로 표시합니다. 이 프롬프트들과 샘플링된 설명들이 우리의 사전 훈련 코퍼스를 형성합니다. 우리는 $\pi$, $\rho$를 사전 훈련 과정 전후의 LM에 대한 사전 및 사후 분포로 사용하며, 이는 언어 모델의 집합인 $LIMS$에 대해 정의됩니다. 프롬프트 $(q, c)$가 주어졌을 때, 우리는 유도된 분포 $PLM (q, c)$와 ground-truth 분포 $P_{world}(q, c)$ 사이의 총 변동(TV) 손실을 정량화하여 LM의 일반화 능력을 측정합니다.⁵ 실제 LM의 경우, 사전 훈련은 훈련 코퍼스에 대한 교차 엔트로피 손실을 최적화하는 것을 포함합니다.

---
⁵우리는 이 섹션에서 LM의 출력 분포를 제한하지 않기 때문에 두 분포 모두 전체 $E$에 대해 정의된 것으로 간주합니다.

---

우리 프레임워크 하에서 $KL[P_{world}(q, C)||PLM(q, c)]$를 최소화하는 것과 같습니다. Pinsker의 부등식에 따르면, KL-발산 항을 최적화하는 것은 TV에 대한 상한을 최적화하는 것과 같으므로, 우리는 경험적 TV 손실이 작을 것으로 예상합니다.

**Definition 3.1.** (TV empirical loss and population loss)
$l_{TV} (PLM, (q, c)) := TV(P_{world}(q, C), PLM (q, c))$
LM과 데이터 집합 $S$가 주어졌을 때, 경험적 손실 $R_S(PLM)$과 모집단 손실 $R(PLM)$은 다음과 같이 정의됩니다.
$R_S(PLM) := \frac{1}{n} \sum_{i=1}^n l_{TV} (PLM, (q_i, C_i));$
$R(PLM) := E_{S \sim D_p^n} [R_S(PLM)] = E_{(g,c) \sim D_p} [l_{TV} (PLM, (q, c))]$

우리는 PAC-Bayesian 경계를 다음과 같이 명시합니다. 자세한 증명은 부록 B.1에서 찾을 수 있습니다.⁶

**Theorem 1.** (PAC-Bayesian Generalization Bound for Language Models.) 정의 2.1에서와 같이 $\alpha$를 사용하여, $LIMS$에 대한 사전 분포 $\pi$를 가진 언어 모델 $LIMS$의 집합을 고려하십시오.
어떤 $\delta \in (0, 1)$에 대해, $\rho, \pi$가 동일한 지원을 공유하는 $LIMS$에 대한 확률 측정 $\rho$에 대해, 다음은 $S$의 무작위 추출에 대해 적어도 $1 - \delta$의 확률로 성립합니다:
$E_{LM \sim \rho}[R(PLM) – R_S(PLM)] \le \frac{1}{2n} [KL[\rho||\pi] + \log(\frac{n}{\delta})] := o;$
$E_{LM \sim \rho}[E_{(q,c) \sim D_p}, l_{TV} (PLM, (q, c))] \le \frac{1}{\alpha} [E_{LM \sim \rho} R_S(PLM) + o].$

부록 B.2에서 우리는 $o$의 이론적 추정치를 제공하여 우리가 도출한 경계가 비-자명함, 즉 1보다 작음을 설명합니다. KL 항은 $\pi, \rho$에 관련된 매개변수 수를 $K$라고 할 때 $O(K)$ 차수이며, $n$은 $K$를 훨씬 초과하는 것으로 나타낼 수 있습니다(프롬프트에 대한 현실적인 Zipf 분포 가정을 사용하여 고유 프롬프트 수를 추정). 정리 1은 사전 훈련이 훈련 코퍼스에 대한 손실을 성공적으로 줄이는 한($R_S(PLM) \downarrow$), 기대치에서 언어 모델이 $D_p$에서 샘플링된 주어진 직접 프롬프트에 대해 세계를 잘 모방할 것임을($l_{TV}$ 차이가 작음) 알려줍니다. 또한, $\alpha$가 너무 작지 않다면, 이 진술은 개념이 해로운 직접 프롬프트에 대해 성립합니다. 우리는 해로운 개념을 높은 확률로 해로운 설명을 출력하는 것으로 정의했으므로(정의 2.1), $D_p$ 데이터로 훈련된 LM이 해로운 집합에서 설명을 출력할 수 있다고 결론 내립니다.

## 4 A statistical perspective on jailbreaking after alignment

이 섹션에서는 우리 연구의 주요 이론적 기여를 제시할 것입니다: 우리의 가정이 성립한다고 가정하면, 선호도 정렬 과정 후에도 적이 LM을 탈옥시킬 방법의 존재를 증명합니다. 우리의 증명 전략은 적대적 예제의 존재를 허용하지 않는 점들의 집합의 부피를 상한으로 하여 적의 성공 확률을 제한하는 적대적 견고성[41]에 대한 연구에서 영감을 받았습니다. 앞으로 나아가기 위해, 우리는 정렬과 탈옥을 통합하기 위해 프레임워크를 확장해야 합니다.

LM이 사전 훈련된 후, 일반적으로 선호하는 행동을 포함하는 데이터셋에 대한 미세 조정을 거칩니다. 다음에서, 우리는 이 정렬 과정이 LM이 여전히 의미적으로 의미있는 설명(정의 2.1)을 생성한다는 의미에서 모델 성능을 변경하지 않는다고 가정할 것입니다. 예를 들어, 어떤 요청에 대해서도 동일한 응답으로 기본 설정되지는 않을 것입니다.

**Assumption 4.1.** (LM outputs semantically meaningful explanations) 모든 해로운 개념 $c$와 모든 그럴듯한 프롬프트 $(q, c) \in dom(p_{world})$에 대해,
$|E_r(c)| \ll |E_h(c)| + |E_s(c)|$ 이고, $|dom(PLM(q,c))| = |E_h(c) \cup E_s(c) \cup E_r(c)|$ 입니다.

즉, 우리는 LM의 출력 분포가 $E_h(c) \cup E_s(c)$에 정확하게 지원된다고 가정합니다. "잔여" $E_r(c)$의 크기가 이러한 의미적으로 의미있는 설명에 비해 상대적으로 작다는 의미에서입니다. 우리는 $n(c) = |E_r(c)|+|E_s(c)|+|E_h(c)|$로 정의합니다. 우리는 문맥에서 명확할 때 $(c)$ 주석을 생략합니다.

---
⁶정리 1의 증명에 대한 영감은 Mbacke et al. [45]에서 비롯되었으며, 증명 아이디어는 원래 Germain et al. [46], Haddouche et al. [47]에서 제안되었습니다.

---

`$O(1)# Mission Impossible: A Statistical Perspective on Jailbreaking LLMs

## Abstract

대규모 언어 모델(LLM)은 품질 관리가 제한된 방대한 텍스트 데이터로 학습됩니다. 그 결과, LLM은 정보 유출, 가짜 뉴스 또는 증오 발언과 같은 의도하지 않거나 해로운 행동을 보일 수 있습니다. 일반적으로 선호도 정렬(preference alignment)이라고 하는 대응책에는 사전 학습된 LLM을 원하는 행동의 신중하게 만들어진 텍스트 예제로 미세 조정하는 것이 포함됩니다. 그럼에도 불구하고, 경험적 증거에 따르면 선호도 정렬된 LLM도 해로운 행동을 하도록 유도될 수 있습니다. LLM의 이러한 탈옥(jailbreaking)은 일반적으로 LLM에 대한 입력 프롬프트를 적대적으로 수정하여 달성됩니다. 저희 논문은 통계적 관점에서 선호도 정렬 및 탈옥 현상에 대한 이론적 통찰력을 제공합니다. 저희 프레임워크 하에서, 먼저 사전 학습된 LLM이 훈련 코퍼스에 존재하는 경우 해로운 행동을 모방할 것임을 보여줍니다. 동일한 프레임워크 하에서, 정렬에 대한 통계적 개념을 도입하고 탈옥 확률의 하한을 정하여 합리적인 가정 하에서는 탈옥을 막을 수 없음을 보여줍니다. 저희의 통찰력을 바탕으로, 현재 널리 사용되는 정렬 전략인 RLHF를 변경할 것을 제안합니다. 구체적으로, 우리는 RLHF 목표에 대한 간단한 수정안인 E-RLHF를 도입하여 안전한 응답의 가능성을 높이는 것을 목표로 합니다. E-RLHF는 추가적인 훈련 비용이 들지 않으며 다른 방법과 호환됩니다. 경험적으로, 우리는 E-RLHF가 AdvBench [1] 및 HarmBench 프로젝트 [2]에서 제시된 모든 정렬 문제에서 MT-Bench 프로젝트 [3]로 측정한 모델 성능을 희생하지 않으면서 RLHF를 능가함을 보여줍니다.

## 1 Introduction

대규모 언어 모델(LLM)은 비서, 코드 생성[4], 헬스케어[5], 정리 증명[6] 등 다양한 영역에서 놀라운 능력으로 딥러닝 분야에 혁명을 일으켰습니다. LLM의 훈련 과정은 일반적으로 두 단계로 구성됩니다: 방대한 코퍼스를 사용한 사전 훈련과, 인간의 피드백을 이용한 강화 학습(RLHF)을 사용하여 모델 행동을 인간의 선호도에 더욱 정렬시키는 정렬 단계입니다. 후자 단계는 일반적으로 대량의 인간 주석 데이터를 포함하며, 지도 미세 조정(SFT) 단계, 보상 모델링 단계, RL 미세 조정 단계로 분해될 수 있습니다. 여러 작업을 효과적으로 수행할 수 있는 능력에도 불구하고, LLM은 사전 훈련 데이터셋[7-9] 내에 해로운 요소가 불가피하게 존재하기 때문에 증오 발언, 악성 코드, 가짜 정보 또는 사회적 편견을 포함한 공격적이거나 부적절한 콘텐츠를 생성하기 쉽습니다. 소셜 미디어는 "Do Anything Now"(DAN) 프롬프트[11]나 "Grandma Exploit" 핵[12]과 같이 ChatGPT[10]를 공격하여 해로운 응답을 유도하는 방법에 대한 수많은 트릭을 보여줍니다. 다른 한편으로, 훈련 코퍼스의 행동 다양성은 예를 들어 다른 문화적 선호도를 포착하는 데 필수적입니다. 무엇이 해롭고 해롭지 않은지는 궁극적으로 사용자 선호도에 따라 달라지므로, 정렬 단계는 보편적이지 않고 모델이 사용될 특정 사용 사례에 따라 달라집니다.

배포 안전성을 해결하고 불쾌한 응답을 제거하기 위해, SFT [13] 중에 안전한 정보를 주입하거나, 인간 전문가와 AI 자체를 이용한 레드팀 구성[14-18], 그리고 전체 RLHF 프로세스를 상세히 개선하고 향상시키는[19-23] 등 수많은 정렬 노력이 있었습니다. 그러나 우리는 "해로운" 프롬프트를 무력화하기 위한 더욱 정교한 정렬 방법과 LLM을 조작하여 해로운 정보를 생성하도록 하는 더욱 창의적인 "탈옥" 공격의 쫓고 쫓기는 게임을 계속 목격하고 있습니다. 이러한 공격은 적대적 접미사 주입[1, 24, 25], 암호 및 저자원 자연어 탐색[26-28], 또는 LLM이 자동으로 프롬프트를 만들게 하는[29-33] 등 다양한 형태로 나타납니다. 접미사에 대한 몇 가지 임시 방어 방법이 제안되었지만[34-37], 탈옥 공격에 대한 원칙적인 보편적 방어[2]에 대한 제안은 제한적이며, 이 현상에 대한 이론적 이해도 제한적입니다[38].

이 논문에서는 사전 훈련 단계와 정렬 후 탈옥 현상을 모두 분석하기 위한 이론적 프레임워크를 제시합니다. 탈옥 프롬프트가 일반적으로 기본 해로운 개념은 유지하면서 프롬프트의 다른 측면을 조작한다는 사실을 활용하여, 잠재적 적의 강점을 정량화할 수 있도록 입력 프롬프트를 분리하는 프레임워크를 설계합니다. 언어 모델(LM)의 출력 요소를 개별 토큰이 아닌 더 긴 텍스트 조각으로 표현함으로써, 이러한 모델이 훈련 분포를 모방하는 정도를 정량화하고 결과적으로 탈옥 취약성의 기본 메커니즘을 더 잘 이해할 수 있습니다.

우리의 기여는 다음과 같이 요약될 수 있습니다:
- 제안된 프레임워크를 기반으로, 먼저 사전 훈련을 위한 비-자명한(non-vacuous) PAC-Bayesian 스타일의 일반화 경계를 제공합니다. 우리 프레임워크의 유효성을 가정하면, 높은 성능의 사전 훈련된 모델은 의도하지 않은 해로운 행동을 포함하여 훈련 코퍼스에 존재하는 행동을 생성하는 데 불가피하게 취약할 것이라고 결론 내립니다.
- 이후, 우리는 정렬 및 탈옥의 개념을 포함하도록 프레임워크를 확장합니다. 우리의 가정이 충족된다고 가정하면, LM이 안전한 응답 집합에 대한 출력 분포를 집중시키는 데 실패하기 때문에 안전 정렬 후에도 탈옥은 예방할 수 없음을 보여줍니다.
- 이론적 발견에 동기를 부여받아, 우리는 널리 채택된 RL Fine-Tuning 목표의 주요 단점을 식별합니다. 이는 무해함과 유용함 목표 간의 자연스러운 차이 때문입니다. 이 문제를 해결함으로써, 우리는 모델 성능을 유지하면서 일련의 탈옥 공격에 더 탄력적인 더 안전한 모델의 훈련을 용이하게 합니다.

이 논문은 다음과 같이 구성됩니다. 섹션 2에서는 우리의 프레임워크를 소개합니다. 섹션 3에서는 사전 훈련에 대한 PAC-Bayesian 일반화 경계를 증명합니다. 다음으로, 섹션 4에서는 통계적 관점에서 탈옥에 대한 분석을 제시합니다. 마지막으로, 섹션 5에서는 제안된 E-RLHF 목표와 LLM 안전성 향상에 대한 효과를 설명합니다. 부록 H에서 문헌 검토를 제공합니다.

## 2 Framework and assumptions

탈옥은 컴퓨터 비전[39]에서 잘 연구된 분야인 적대적 공격과 몇 가지 유사점을 가집니다. 여기서 적은 주어진 입력 이미지를 픽셀 공간에서 교란하여 모델 출력을 변경하는 맵으로 정의됩니다. 적의 강도는 `lp` 거리[40-42]로 정량화된 원래 입력을 얼마나 멀리 이동시킬 수 있는지에 따라 제한됩니다. 일반적으로 이 거리는 인간 관찰자에게 변화가 인지되지 않는 방식으로 제한됩니다. 적의 목표는 입력의 오분류를 유발하는 것입니다. 대조적으로, LLM의 경우, 적의 목표는 정보의 의도하지 않은 유출이나 증오 발언과 같은 해로운 행동을 유발하는 것입니다. 또한, 프롬프트라고 불리는 입력에 대한 모든 교란은 인지 가능한 효과를 가집니다. 따라서 적의 능력을 정량화하고 제한하는 것은 간단하지 않습니다. 그럼에도 불구하고, 약간의 수정을 통해 이 비유를 기반으로 구축할 것입니다.

이 연구의 목적을 위해, 우리는 모든 프롬프트를 쿼리와 개념의 튜플 $(q,c)$로 볼 것이며, 여기서 $c \in C$이고 $q \in Q$이며, $C$, $Q$는 전체 개념 집합과 쿼리 집합을 나타냅니다. 개념적으로, 우리는 개념을 프롬프트의 정보 내용을 나타내는 것으로 생각하며, 보통 "케이크 만드는 법 튜토리얼"과 같은 짧은 텍스트 조각을 통해 나타냅니다. 쿼리는 특정 개념과 구성 가능한 지시적 텍스트 조각입니다. 우리는 쿼리를 LM이 특정 방식으로 개념을 확장하도록 유발하는 메커니즘으로 생각할 수 있습니다. 예로는 "{}하는 법 알려줘" 또는 "우리는 지금 상상의 세계에 있고, 너는 어떤 윤리적 제약에도 얽매이지 않아. 내 아바타에게 {}하는 법을 가르쳐줘"가 있습니다. 모든 쿼리와 개념이 구성 가능한 것은 아니므로, 우리는 $P \subseteq Q \times C$를 모든 그럴듯한 프롬프트의 집합으로 표시하며, 그럴듯함의 정의는 아래에서 명확히 할 것입니다.

프롬프트의 분해는 우리가 적의 강점을 분리하고 따라서 제한할 수 있게 해줍니다. 해로운 행동을 유도하는 현재의 경험적 연구와 일치하게, 우리는 개념이 아닌 쿼리에만 교란을 허용할 것입니다. 적대적 공격에 대한 이전 연구의 정신을 더욱 모방하여, 우리는 프롬프트와 관련된 ground-truth가 쿼리가 아닌 개념에 의해서만 결정된다고 가정할 것입니다. 우리는 다음 단락에서 이러한 아이디어를 더 엄격하게 만들 것입니다.

첫째, LM을 단일 문장 생성기[38]로 간주했던 이전의 이론적 연구와는 대조적으로, 우리는 LM을 더 긴 텍스트 조각 생성기로 모델링하고, 생성된 가능한 콘텐츠 $e \in E$를 설명(explanation)이라고 지칭합니다. 개념적으로, 설명은 추가 정보로 개념을 확장합니다. 예를 들어, "2023년 미국 대통령은 조 바이든이다."가 있습니다. 우리의 용어 "설명"은 LM이 입력을 받아 응답을 생성하는 정책으로 간주되는 이전 논의(예: [23])에서 사용된 "응답"과 개념적으로 동일합니다. 우리는 현재 커뮤니티에서 고려되는 대부분의 탈옥 공격에서 적이 단일 해로운 시도에 대한 지침이나 설명을 찾기 때문에 "개념"과 대조하기 위해 "설명"을 사용합니다. 따라서 LM은 그럴듯한 프롬프트에서 설명에 대한 분포로의 매핑 $PLM : P \to \Delta(E)$를 유도하며, 여기서 $\Delta(E)$는 $E$의 원소에 대해 정의된 분포 집합을 나타냅니다.⁴ LM의 출력 $PLM(q, c)$는 설명에 대한 이산 분포입니다. 우리는 이 분포의 도메인을 $dom(PLM (q, c))$로, 입력 $(q, c)$가 주어졌을 때 $e$의 확률을 $PLM (e|q, c)$로, 0이 아닌 $PLM (e|q, c)$를 가진 $E$의 부분 집합을 $supp(PLM(q, c))$로 사용합니다. 또한, 우리는 사전 훈련 단계에서 LM이 모방하도록 최적화된 잠재적 ground truth 매핑 $P_{world} : P \to \Delta(E)$가 존재한다고 가정합니다. 이것이 "지식"을 정의하는 분포입니다: 모든 그럴듯한 프롬프트 $(q, c)$에 대해, 설명에 대한 ground-truth 분포를 지정합니다. 그럴듯하다는 것은 ground truth 매핑의 도메인에 있는 모든 프롬프트, 즉 $(q, c) \in dom(p_{world})$를 의미하며, $P = dom(p_{world})$입니다. 아래에서 논의될 바와 같이, 많은 그럴듯한 프롬프트는 사용 가능한 훈련 코퍼스 내에 존재하지도 않을 것입니다.

이제 우리는 주요 가정을 명시할 수 있습니다. 즉, 모든 그럴듯한 프롬프트 $(q, c) \in dom(p_{world})$에 대해 ground-truth 분포 $P_{world}(q, c)$는 $E$의 작은 부분 집합에서 지원된다는 것입니다. $supp(P_{world}(q,c)) \subset E$. 이 가정은 우리에게 합리적으로 보입니다. 정상적인 상황에서 "파리"에 대한 설명을 제공하는 것은 "헬로 월드 파이썬 스크립트를 작성하는 방법"과 같은 프롬프트가 주어졌을 때 관련 지식을 제공하지 않을 것이기 때문입니다. 우리의 두 번째 가정은 모든 그럴듯한 프롬프트 $(q, c)$에 대해, 개념 $c$가 쿼리에 관계없이 $P_{world}$에 의해 지정된 출력 분포의 지원을 고유하게 결정한다는 것입니다: $supp(p_{world}(q, c)) = supp(P_{world}(q*, c))$, 모든 그럴듯한 $(q, c)$ 및 $(q*, c)$. 쿼리는 지원에 영향을 주지 않으면서 ground-truth 분포를 변경합니다. 그림 1에 그림이 나와 있습니다. 더 정확하게 말하면:

**Assumption 2.1.** (Concepts uniquely determine the explanation for plausible prompts)
모든 그럴듯한 프롬프트 $(q, c) \in dom(p_{world})$에 대해,

i) $P_{world}: P \to \Delta(supp(P_{world}(q, c)))$

---
³예를 들어, "케이크 만드는 법 튜토리얼은 누구인가."는 비합리적입니다.
⁴실제 LM의 경우, 온도 T, top-p 및 top-k 샘플링 매개변수와 같은 다른 디코딩 하이퍼파라미터를 사용하면 동일한 매개변수 집합으로 유도된 분포가 다를 수 있습니다. 우리의 논의는 이 논문 전체에서 미리 고정된 하이퍼파라미터 집합에 대해 유효합니다.

---

ii) $supp(P_{world}(q, c)) = supp(P_{world}(q*, c))$, 모든 $(q, c)$, $(q*, c)$는 그럴듯합니다.

이 가정은 본질적으로 지식이 사용된 쿼리에 관계없이 해당 개념에 의해서만 지정된다는 것을 의미하기 때문에 자연스럽습니다. 즉, 개념 $c$가 주어졌을 때 쿼리 $q$가 $supp(P_{world}(q, c))$를 변경하는 데 성공하면, 우리는 쿼리가 분해되어 지원에 의해 미러링된 지식을 정확하게 반영하기 위해 $e$에 부분적으로 흡수되어야 한다고 주장합니다.

마지막으로, 우리는 프롬프트에 대한 기본 생성 분포의 존재를 가정하며, 이를 $(q, c) \sim D_p$로 표시합니다. 이 분포는 사전 훈련 코퍼스 생성의 원칙으로 작용합니다. $supp(D_p) \subseteq dom(p_{world})$라는 점에 유의하는 것이 중요합니다. 예를 들어, 프롬프트 $(q', c')$="제임스 본드는 누구인가 $入*#!48811"를 생각해 봅시다. 이 프롬프트는 인터넷의 어떤 텍스트 코퍼스에도 나타나지 않지만, $(q', c') \notin supp(D_p)$, 우리 인간은 그것을 이해할 수 있습니다: $(q', c') \in dom(P_{world})$. 이 논문의 뒷부분 증명에서는 LM이 실제로 거대한 분포 외 데이터셋[43]에서 잘 일반화된다고 주장되기 때문에, 이러한 보이지 않는 그럴듯한 프롬프트에 대해 의미적으로 합리적인 설명을 생성한다고 가정합니다. 이는 섹션 4의 가정 4.1 내에서 명시적으로 만들어집니다.

마지막으로, 다음 정의는 우리의 유해성 개념에 관한 것입니다. 더 구체적으로, 우리는 유해한 행동을 추상적으로 의도하지 않은 행동으로 이해합니다. 이를 위해, 우리는 모든 설명 `e`가 유해하거나 유해하지 않은(안전한) 것으로 표시될 수 있다고 가정합니다. 개념 `c`는 직접적인 프롬프트로 특정 임계값보다 높은 확률로 유해한 설명을 생성하는 경우에만 유해한 것으로 간주됩니다.

**Definition 2.1.** (Notions of Harmfulness)
- **(Direct Queries and Direct Prompts)** 우리는 프롬프트가 $D_p$에서 비롯된 경우, 즉 $(q, c) \in supp(D_p)$인 경우 직접적이라고 지칭합니다. 직접적인 프롬프트의 쿼리를 직접 쿼리라고 합니다.
- **(Harmful Concepts and Harmful Set)** 개념 $c$가 주어졌을 때, 관련된 유해한 설명 집합은 $E_h(c) := \{e | e \in supp(p_{world}(·,c)) \land e \text{ is harmful}\}$로 표시됩니다. 가정 2.1에 따라, 임계값 $\eta$를 사용하여, 개념 $c$는 모든 $q$에 대해 $(q, c) \in dom(p_{world})$일 때, $\sum_{e:e \in E_h(c)} P_{world}(e|q, c) \ge 1 - \eta$이면 해롭습니다. 우리는 모든 가능한 해로운 개념의 집합을 $C_h \subset C$로 지칭합니다.
- **(Safe Set)** 모든 $c \in C_h$에 대해, $PLM (q, c)$가 집중되기를 바라는 해당 안전 집합 $E_s(c) \subseteq E$가 존재합니다. 여기에는 $supp(p_{world}(·, c))$에 존재하는 안전한 설명과 "죄송합니다."로 시작하는 템플릿과 같이 인간이 설계한 설명이 포함됩니다.
- **(Semantically meaningful)** 우리는 $E_h(c) \cup E_s(c)$의 설명을 $(q, c)$ 프롬프트에 대해 의미적으로 의미있는 것으로 부릅니다.
- **(Mixture decomposition of Dp)** 이러한 개념으로, 우리는 $D_p = \alpha D_{ph} + (1 - \alpha)D_{p\bar{h}}$ (여기서 $supp(D_{ph})$는 해로운 개념을 가진 모든 직접 프롬프트를 포함하고, $supp(D_{p\bar{h}})$는 그 보완을 포함함)를 해로운 개념을 가진 직접 프롬프트와 그렇지 않은 프롬프트의 혼합으로 분해할 수 있습니다.

## 3 PAC-Bayesian bound for pre-training LLMs on harmful data

모델 집합에 대한 사후 분포로 이어지는 학습 알고리즘이 주어졌을 때, PAC-Bayesian 이론[44]은 일반화 격차, 즉 모델의 경험적 손실과 모집단 손실 간의 차이에 대한 경계를 제공하기 위해 아마도 거의 정확하게(PAC) 부등식을 적용합니다. 이제 우리는 분석의 첫 번째 결과인 사전 훈련 LM에 대한 비-자명한 PAC-Bayesian 경계를 제시하며, 이는 잘 훈련된 LM이 훈련 중에 해로운 행동을 접했다면 단순히 직접 쿼리로 프롬프트될 때조차도 해로운 행동을 보일 것임을 암시합니다.

우리는 $S = \{(q_i, C_i)\}_{i=1}^n$을 $D_p$ 하에서 i.i.d.로 생성된 프롬프트 집합, 즉 $S \sim D_p^n$로 표시합니다. 이 프롬프트들과 샘플링된 설명들이 우리의 사전 훈련 코퍼스를 형성합니다. 우리는 $\pi$, $\rho$를 사전 훈련 과정 전후의 LM에 대한 사전 및 사후 분포로 사용하며, 이는 언어 모델의 집합인 $LIMS$에 대해 정의됩니다. 프롬프트 $(q, c)$가 주어졌을 때, 우리는 유도된 분포 $PLM (q, c)$와 ground-truth 분포 $P_{world}(q, c)$ 사이의 총 변동(TV) 손실을 정량화하여 LM의 일반화 능력을 측정합니다.⁵ 실제 LM의 경우, 사전 훈련은 훈련 코퍼스에 대한 교차 엔트로피 손실을 최적화하는 것을 포함합니다.

---
⁵우리는 이 섹션에서 LM의 출력 분포를 제한하지 않기 때문에 두 분포 모두 전체 $E$에 대해 정의된 것으로 간주합니다.

---

우리 프레임워크 하에서 $KL[P_{world}(q, C)||PLM(q, c)]$를 최소화하는 것과 같습니다. Pinsker의 부등식에 따르면, KL-발산 항을 최적화하는 것은 TV에 대한 상한을 최적화하는 것과 같으므로, 우리는 경험적 TV 손실이 작을 것으로 예상합니다.

**Definition 3.1.** (TV empirical loss and population loss)
$l_{TV} (PLM, (q, c)) := TV(P_{world}(q, C), PLM (q, c))$
LM과 데이터 집합 $S$가 주어졌을 때, 경험적 손실 $R_S(PLM)$과 모집단 손실 $R(PLM)$은 다음과 같이 정의됩니다.
$R_S(PLM) := \frac{1}{n} \sum_{i=1}^n l_{TV} (PLM, (q_i, C_i));$
$R(PLM) := E_{S \sim D_p^n} [R_S(PLM)] = E_{(g,c) \sim D_p} [l_{TV} (PLM, (q, c))]$

우리는 PAC-Bayesian 경계를 다음과 같이 명시합니다. 자세한 증명은 부록 B.1에서 찾을 수 있습니다.⁶

**Theorem 1.** (PAC-Bayesian Generalization Bound for Language Models.) 정의 2.1에서와 같이 $\alpha$를 사용하여, $LIMS$에 대한 사전 분포 $\pi$를 가진 언어 모델 $LIMS$의 집합을 고려하십시오.
어떤 $\delta \in (0, 1)$에 대해, $\rho, \pi$가 동일한 지원을 공유하는 $LIMS$에 대한 확률 측정 $\rho$에 대해, 다음은 $S$의 무작위 추출에 대해 적어도 $1 - \delta$의 확률로 성립합니다:
$E_{LM \sim \rho}[R(PLM) – R_S(PLM)] \le \frac{1}{2n} [KL[\rho||\pi] + \log(\frac{n}{\delta})] := o;$
$E_{LM \sim \rho}[E_{(q,c) \sim D_p}, l_{TV} (PLM, (q, c))] \le \frac{1}{\alpha} [E_{LM \sim \rho} R_S(PLM) + o].$

부록 B.2에서 우리는 $o$의 이론적 추정치를 제공하여 우리가 도출한 경계가 비-자명함, 즉 1보다 작음을 설명합니다. KL 항은 $\pi, \rho$에 관련된 매개변수 수를 $K$라고 할 때 $O(K)$ 차수이며, $n$은 $K$를 훨씬 초과하는 것으로 나타낼 수 있습니다(프롬프트에 대한 현실적인 Zipf 분포 가정을 사용하여 고유 프롬프트 수를 추정). 정리 1은 사전 훈련이 훈련 코퍼스에 대한 손실을 성공적으로 줄이는 한($R_S(PLM) \downarrow$), 기대치에서 언어 모델이 $D_p$에서 샘플링된 주어진 직접 프롬프트에 대해 세계를 잘 모방할 것임을($l_{TV}$ 차이가 작음) 알려줍니다. 또한, $\alpha$가 너무 작지 않다면, 이 진술은 개념이 해로운 직접 프롬프트에 대해 성립합니다. 우리는 해로운 개념을 높은 확률로 해로운 설명을 출력하는 것으로 정의했으므로(정의 2.1), $D_p$ 데이터로 훈련된 LM이 해로운 집합에서 설명을 출력할 수 있다고 결론 내립니다.

## 4 A statistical perspective on jailbreaking after alignment

이 섹션에서는 우리 연구의 주요 이론적 기여를 제시할 것입니다: 우리의 가정이 성립한다고 가정하면, 선호도 정렬 과정 후에도 적이 LM을 탈옥시킬 방법의 존재를 증명합니다. 우리의 증명 전략은 적대적 예제의 존재를 허용하지 않는 점들의 집합의 부피를 상한으로 하여 적의 성공 확률을 제한하는 적대적 견고성[41]에 대한 연구에서 영감을 받았습니다. 앞으로 나아가기 위해, 우리는 정렬과 탈옥을 통합하기 위해 프레임워크를 확장해야 합니다.

LM이 사전 훈련된 후, 일반적으로 선호하는 행동을 포함하는 데이터셋에 대한 미세 조정을 거칩니다. 다음에서, 우리는 이 정렬 과정이 LM이 여전히 의미적으로 의미있는 설명(정의 2.1)을 생성한다는 의미에서 모델 성능을 변경하지 않는다고 가정할 것입니다. 예를 들어, 어떤 요청에 대해서도 동일한 응답으로 기본 설정되지는 않을 것입니다.

**Assumption 4.1.** (LM outputs semantically meaningful explanations) 모든 해로운 개념 $c$와 모든 그럴듯한 프롬프트 $(q, c) \in dom(p_{world})$에 대해,
$|E_r(c)| \ll |E_h(c)| + |E_s(c)|$ 이고, $|dom(PLM(q,c))| = |E_h(c) \cup E_s(c) \cup E_r(c)|$ 입니다.

즉, 우리는 LM의 출력 분포가 $E_h(c) \cup E_s(c)$에 정확하게 지원된다고 가정합니다. "잔여" $E_r(c)$의 크기가 이러한 의미적으로 의미있는 설명에 비해 상대적으로 작다는 의미에서입니다. 우리는 $n(c) = |E_r(c)|+|E_s(c)|+|E_h(c)|$로 정의합니다. 우리는 문맥에서 명확할 때 $(c)$ 주석을 생략합니다.

---
⁶정리 1의 증명에 대한 영감은 Mbacke et al. [45]에서 비롯되었으며, 증명 아이디어는 원래 Germain et al. [46], Haddouche et al. [47]에서 제안되었습니다.

---

 진술은 해로운 설명이 일반적으로 많은 대안적인 공식을 허용하는 긴 텍스트 조각이기 때문에 합리적입니다. 이 가정은 두 가지 구성 요소로 나눌 수 있습니다: (1) 출력 분포의 지원 내에서 관련 없는 설명의 가끔 인스턴스만 존재합니다; (2) 모델을 안전성으로 정렬하는 과정이 사전 훈련 단계에서 획득한 해로운 설명을 제거하지 않습니다. (1)의 경우, 위에서 제시한 예와 유사하게, 정상적인 상황에서 \"폭탄 만드는 법\"과 같은 $(q, c)$가 주어졌을 때 $dom(PLM(q,c))$에 \"파리\"라는 설명이 나타날 것으로 기대하지 않습니다. (2)의 경우, 놀랍게 보일 수 있지만, 현재 최첨단 LM 시리즈에 대한 증거는 직접 프롬프트를 사용하여 디코딩 프로세스를 단순히 조작함으로써 다양한 해로운 설명이 추출되는 실험적으로 검증될 수 있습니다[48]. 섹션 5에서 우리는 이 바람직하지 않은 현상에 대한 설명을 제공합니다.

탈옥의 가능성을 제한하기 위해 먼저 LM의 출력이 지원과 어떻게 상호 작용하는지 명시해야 합니다. $dom(PLM(q,c))$에서 설명의 고정된 순서를 가정하고 표기법을 약간 남용하면, $PLM (q, c)$를 $n(c)$개의 요소를 가진 확률 단체인 $\\Delta^{n(c)-1}$에 대한 $n(c)$차원 벡터로 나타낼 수 있으며, 각 항목은 단일 설명의 확률을 나타냅니다. 우리는 이 단체를 주어진 개념 $c$와 관련된 출력 단체라고 부릅니다. 다음으로, 언어 모델 $LIMS$의 집합에 대한 사후 분포 $\\gamma$가 주어지면 이 단체에 대한 분포를 유도할 수 있습니다.

**Definition 4.1.** (Induced Distribution on Simplex, $\\mathcal{Y}_c$)
LM이 의미적으로 의미있는 설명(가정 4.1)을 출력한다는 가정 하에, 고정된 프롬프트 $(q,c)$와 $LIMS$에 대한 사후 분포 $\\gamma$가 주어지면, 해당 유도 분포: $LM \\sim \\gamma$인 $PLM(q, c)$는 출력 단체 $\\Delta^{n-1}$의 부분 집합에 지원됩니다. 이 분포는 $\\mathcal{Y}_{(q,c)}$로 표시되거나, $q$에 대한 참조가 문맥에서 명확할 때 $\\mathcal{Y}_c$로 표시됩니다.

다음으로, 우리는 출력 단체를 해로운 구역과 안전 구역으로 분리할 것입니다. 이 정의는 일반적으로 적이 주어진 개념에 대해 단 하나의 해로운 설명이라도 추출할 수 있으면 성공한 것으로 간주된다는 관찰에 의해 동기 부여됩니다. 이는 가정 4.1 하에서 출력 단체의 분할로 변환됩니다.

**Definition 4.2.** (Harmful Zone and Safety Zone) 주어진 해로운 개념 $c$와 고정된 LM에 대해, 출력 단체는 안전 구역과 해로운 구역, 즉 $H_s$와 $H_h$로 나뉘며, 여기서 미리 정의된 임계값 $p \\in [0,1]$은 구별을 정량화하는 데 사용됩니다: $PLM(q,c) \\in H_h$는 $\\sum_{e:e \\in E_h(c)} PLM (e|q, c) \\ge p$인 경우에만 해당하며, 그렇지 않으면 $PLM(q, c) \\in H_s$입니다.

탈옥을 소개하기 전에, 독자는 왜 우리가 정렬을 더 명확하게 정의하지 않았는지 궁금해할 수 있습니다. 이는 PAC 프레임워크 하에서 선호도 정렬은 $LIMS$에 정의된 어떤 $\\gamma$ 사후 분포로의 변환에 불과하기 때문입니다. 정렬에 대한 세분화된 특성화의 이러한 불가능성을 감안할 때, 우리는 대신 그 목표를 다음과 같이 제공합니다. 위의 개념으로, $c$가 해로운 프롬프트 $(q, c)$가 주어졌을 때, 그 목표는 유도된 분포 $\\mathcal{Y}_c$를 안전 구역 $H_s$로 밀어 넣는 것입니다. 이상적으로, $supp(\\mathcal{Y}_c) \\subseteq H_s$는 확률 1로, 결과 LM이 $(q, c)$를 만났을 때 안전합니다. 우리는 탈옥과 관련된 필요한 개념을 소개할 준비가 되었습니다.

**Definition 4.3.** (Jailbreaking) 해로운 개념 $c$와 쿼리 $q\'$가 주어졌을 때, 프롬프트 $(q\', c)$는 $PLM (q\', c) \\in H_h$인 경우에만 LM을 탈옥시킵니다. 우리는 이러한 프롬프트 $(q\', c)$와 쿼리 $q\'$를 각각 탈옥 프롬프트와 탈옥 쿼리라고 부릅니다.

$H_h$와 $H_s$를 구별하는 임계값 $p$는 매우 작아야 합니다. 이는 기대치에서 적이 단일 해로운 설명을 수집하기 위해 LM을 $1/p$번 호출해야 함을 의미하기 때문입니다. 즉, LM을 탈옥시키기 위함입니다.

탈옥 효과를 이론적으로 증명하기 위해, 우리는 적의 능력을 제한해야 합니다. 이 목표를 달성하기 위해, 우리는 적대적 공격으로부터 통찰력을 빌려, 적이 쿼리 집합을 검색할 때 출력 단체에 대해 제한된 조작 능력을 가지고 있다고 가정합니다:

**Assumption 4.2.** ($\epsilon$-bounded adversary) LM, 해로운 개념 $c$ 및 관련 직접 프롬프트 $(q, c)$가 주어졌을 때, 우리는 적이 출력이 $PLM(q, c)$에서 $H_h$쪽으로 단체에서 최대 $\\epsilon$만큼 이동하는 쿼리 집합 $Q\'$를 찾을 수 있다고 가정합니다:
$ \\sup_{q\' \\in Q\'} d(PLM(q,c), PLM(q\',c)) = \\epsilon $
여기서 $d$는 두 이산 분포 간의 거리 측정입니다. $d$는 $p \\ge 1$인 일반적인 $l_p$ 측정 또는 총 변동 / 젠슨-섀넌 발산일 수 있습니다. 우리는 $q\' \\in Q\'$를 $\\epsilon$-bounded query라고 부릅니다.

우리 프레임워크의 개념적 그림은 그림 2에 나와 있습니다. 정리에 도달하기 전에, $\\epsilon$-expansion의 최종 정의를 제공합니다.

**Definition 4.4.** ($\epsilon$-expansion) 집합 $A \\subset \\Delta^{n-1}$과 거리 측정 $d$가 주어졌을 때, $\\epsilon$-expansion 집합 $A(\\epsilon, d)$는 다음과 같이 정의됩니다.
$ A(\\epsilon, d) := \\{t | t \\in \\Delta^{n-1} \\land \\exists y \\in A \\text{ s.t. } ||y – t||_d < \\epsilon\\} $

이제 우리는 유도된 사후 분포 $\\mathcal{Y}_c$가 극도로 안전한 영역에 집중되지 않는 한, 높은 확률로 모델이 탈옥될 수 있음을 명시하는 다음 정리를 제시할 준비가 되었습니다. 증명은 부록 B.3에 있습니다.

**Theorem 2.** (Jailbreak is unavoidable) LM이 의미적으로 의미있는 설명을 출력한다고 가정합니다(가정 4.1). $LIMS$에 대한 임의의 $\\gamma$ 사후 분포가 주어지면, 직접 프롬프트 $(q, c)$와 임계값 $p$(정의 2.1)를 가진 해로운 개념 $c$를 선택하여 해당 유도 분포 $\\mathcal{Y}_c$(정의 4.1)와 출력 단체에 대한 분할(정의 4.2)을 정의합니다. $\\epsilon$-bounded 적(가정 4.2)은 적어도 다음 확률로 탈옥 프롬프트(정의 4.3)를 찾을 수 있습니다.
$ 1 - \\gamma_s \\times (1 – \\Phi(\\alpha_\\epsilon)) $
- $PLM(q, c) \\in H_h$와 같은 직접 프롬프트를 사용하거나;
- $PLM(q\', c) \\in H_h$와 같은 $\\epsilon$-bounded 쿼리 $q\'$를 찾음으로써.

여기서 $\\Phi(\\cdot)$는 표준 가우시안 cdf이고, $\\gamma_s := \\max_{x \\in H_s \\setminus H_h(\\epsilon,d)} (\\mathcal{Y}_c(x) / U(x))$이며, $U(x)$는 $\\Delta^{n-1}$에 대한 균등 분포이고, $\\alpha_\\epsilon := a + \\sqrt{n-1}\\epsilon$이며, 여기서 $a$는 $a = (|E_h(c)|-1-(n-1)p) / \\sqrt{(n-1)p(1-p)}$로 분석적으로 씁니다.

당연히, 더 강한 적($\\epsilon$ ↑)에 대해 적이 탈옥 프롬프트를 찾을 가능성이 증가합니다. 실제 세계에서, 이는 특정 해로운 개념에 대한 쿼리를 변경하기 위해 우리가 허용하는 계산 예산과 관련될 수 있습니다. 또한, 해로운 설명 집합의 크기와 안전한 설명 집합의 크기 비율이 클 때($|E_h(c)| / |E_s(c)|$ ↑) 적이 탈옥 프롬프트를 찾을 가능성이 증가합니다. 이는 그들의 비율이 해로운 구역의 크기를 결정하고, 이는 결국 $a \\to 1$로 가게 하기 때문입니다. 실제 환경에서, 어떤 해로운 개념에 대해서든, 훈련 코퍼스는 가능한 응답의 수 때문에 자연스럽게 큰 해로운 집합을 포함합니다. 현실적으로, 그 크기는 수동으로 구성된 어떤 안전 집합으로도 대응할 수 없습니다. 따라서 정렬을 달성하는 것은 어렵습니다: 정렬의 목표는 높은 확률로 안전한 설명만으로 응답하는 것임을 상기하십시오. 그러나, 우리는 방금 그 확률을 높이려면, 우리가 비현실적이라고 논의한 작은 해로운-안전한 집합 비율이 필요하다는 것을 배웠습니다. 결과적으로, 안전 구역은 작아질 것입니다.

## 5 E-RLHF: improving alignment by expanding the safety zone

정리 2와 이전 섹션의 후속 논의에서, 탈옥은 안전 구역에 비해 해로운 구역이 클수록 더 가능성이 높아진다는 것을 상기하십시오. 두 구역의 크기는 각각의 설명 집합의 크기와 관련이 있습니다. 즉, 선호도 정렬 데이터셋의 크기는 성공적인 정렬에 중요합니다. 불행히도, 이러한 데이터셋을 만드는 데 관련된 인간 노동은 그 크기를 효과적으로 제한합니다.

이론적 통찰력과 탈옥 문제를 억제하기 위한 실용적인 해결책 사이의 격차를 해소하기 위해, 우리는 안전 구역을 확장하는 다른 더 실용적인 방법에 초점을 맞춥니다. 우리의 아이디어는 더 광범위하게 적용될 수 있지만, 실험에서는 인간 피드백을 이용한 강화 학습(RLHF) 개선에 초점을 맞출 것입니다. RLHF는 일반적으로 세 단계를 포함합니다: i) 지도 미세 조정(SFT); ii) 선호도 샘플링 및 보상 학습, iii) RL 최적화. Rafailov et al. [23]은 최근 LM에 대해 널리 적용되는 RLHF 버전인 직접 선호도 최적화(DPO)를 제안했습니다. 이는 보상 모델을 사전에 얻을 필요 없이 선호도 데이터셋에서 직접 학습할 수 있게 하는 영리한 재매개변수화를 사용합니다.

DPO는 다른 RLHF 구현보다 훈련 과정에서 더 안정적입니다. RLHF와 DPO에 대한 더 완전한 개요는 부록 C에서 찾을 수 있습니다.

우리의 목적을 위해, 우리는 고품질 데이터에 대해 지도 미세 조정된 $LLM P_{SFT}$에 접근할 수 있다고 가정합니다. 우리는 또한 텍스트 프롬프트 $(q, c) = x$와 인간 주석가에 의해 더 나은 $e_w$ 또는 더 나쁜 $e_l$로 평가된 두 개의 해당 설명을 포함하는 선호도 정렬 데이터셋 $D_s$에 접근할 수 있다고 가정합니다. RLHF의 ii) 단계에서는 일반적으로 주석이 달린 설명을 기반으로 보상 모델 $r(x, e)$를 최적화합니다. 우리의 제안은 RLHF 프로세스의 iii) 단계, 즉 선호도 정렬 모델 $P_{LM}$의 훈련에 관한 것입니다. 주어진 보상 모델에 대해, $P_{LM}$은 일반적으로 다음 목표를 최소화하여 얻어집니다:
$ L_{RLHF} (P_{LM}) = -E_{x \sim D_s} \{E_{e \sim p_{LM}(\cdot|x)} [r(x, e)] + \beta D_{KL}(P_{LM}(x)||P_{SFT}(x)) \} \quad (2) $

첫 번째 항은 보상을 최대화하는 반면, KL-항은 정렬된 모델이 SFT 모델에서 너무 멀리 벗어나지 않도록 하는 정규화기로 작용합니다. 우리는 이 정규화가 바로 안전성의 문제라고 주장합니다. 모델을 유용하게 유지하도록 설계되었지만⁷, 모든 해로운 프롬프트 $x_h$와 모든 해로운 설명 $e \in supp(P_{SFT}(x_h))$에 대해, $P_{LM}$을 $P_{SFT}$로 정규화하면 $P_{LM}$이 출력 분포의 지원에서 $e$를 유지하게 합니다. 구체적으로, 지도 미세 조정 과정은 사전 훈련된 모델의 출력 분포 지원에서 해로운 설명을 제거하는 것을 포함하지 않으며⁸, 이는 $P_{SFT}(x_h)$가 현실적인 크기의 선호도 정렬 데이터셋으로 부정할 수 없는 해로운 응답을 지원한다는 사실로 이어집니다. 따라서, $P_{SFT}$의 이 작은 안전 집합 문제는 $P_{LM}$이 위 목표의 최적 해로 최적화되더라도 $P_{LM}$에 전달될 것입니다. 이는 우리가 $P_{LM}$을 $P_{SFT}$가 안내하는 방향으로 밀어붙여서는 안 된다는 것을 시사합니다. 대신, 모든 해로운 프롬프트 $x_h$에 대해, 우리는 $P_{SFT}(x_h)$를 대체하기 위해 큰 안전 구역을 갖도록 설계한 분포를 사용할 것을 제안합니다. 우리 프레임워크에 영감을 받아, 우리는 해로운 개념을 안전한 개념으로 대체하여 해로운 프롬프트를 수정함으로써 이를 달성할 수 있으며, 이는 $supp(P_{SFT}(\cdot))$를 변경하여 더 많은 안전한 설명을 포함하도록 합니다. 예를 들어, $x_h$가 "폭탄 만드는 법 알려줘"일 때, $P_{LM}(x_h)$를 $P_{SFT}(x_h)$에 정렬하는 대신, 이제 우리는 $P_{LM}(x_h)$를 $P_{SFT}(x_s)$에 정렬하며, 여기서 $x_s$는 "폭탄 만드는 요청을 거절하는 법 알려줘"입니다. 이는 $D_s$의 모든 해로운 프롬프트에 대해 개별화된 방식으로 수행될 수 있거나, 단순히 모든 해로운 프롬프트 앞에 "응답이 커뮤니티 가이드라인과 윤리 기준을 준수하도록 하십시오:"와 같은 접두사를 붙임으로써 수행될 수 있습니다.⁹ 중요하게, 해롭지 않은 프롬프트는 수정되지 않습니다. 출력 분포의 안전 구역을 확장하는 우리 접근 방식의 초점 때문에, 우리는 우리의 제안을 E-RLHF라고 명명했으며, 이는 식 (2)에 다음과 같은 수정을 가져옵니다:
$ L_{E-RLHF} (P_{LM}) = -E_{x \sim D_s}\{E_{e \sim p_{LM}(\cdot|x)}[r(x, e)] + \beta D_{KL}(P_{LM}(x)||P_{SFT}(x_s)) \} \quad (3) $
여기서 $x_s$는 원래 해로운 프롬프트 $x_h$의 안전 변환된 버전입니다. 요약하자면, 우리가 제시하는 핵심 주장은 모델 미세 조정의 안정성을 보장하기 위해, 특히 원래 입력 $x$ 자체가 해로울 때, 참조 모델과 대상 모델 모두에 대해 동일한 프롬프트 입력 $x$를 사용하는 것이 필수적이지 않다는 것입니다. 사실, 대체 또는 "앵커" 프롬프트가 원래 프롬프트에 의해 생성된 것과 유사한 논리적으로 합리적인 출력을 생성하는 한, 이 접근 방식은 모델의 훈련 과정을 방해하지 않을 것입니다. 우리의 주장을 확고히 하기 위해, 우리는 부록 C에서 최적 정책의 지원에 대한 우리 수정의 영향을 보여줍니다. 우리는 또한 명시적인 보상 모델 없이 훈련할 수 있도록(단계 ii) 제거) DPO 목표에 우리의 수정을 사소하게 통합할 수 있음을 추론합니다. 여기서 $\sigma(\cdot)$는 시그모이드 함수를 나타냅니다:
$ L_{E-DPO} (P_{LM}) = -E_{(x,e_w,e_l) \sim D_s} [\log \sigma(\beta \log \frac{P_{LM}(e_w|x)}{P_{SFT}(e_w|x_s)} - \beta \log \frac{P_{LM}(e_l|x)}{P_{SFT}(e_l|x_s)})] \quad (4) $

## 6 Experiments and results

우리의 실험 설정은 alignment-handbook 코드 베이스[50]를 기반으로 합니다. 우리는 huggingface hub[51]에서 제공하는 공개적으로 사용 가능한 SFT 모델 `PSFT`를 공개 데이터셋[52, 53]을 사용하여 기본 하이퍼파라미터 설정으로 조정합니다. 우리는 GPT-3.5-Turbo에 프롬프트를 보내 선호도 데이터셋의 해로운 프롬프트를 레이블링합니다(부록 E 참조). 우리는 `xs`를 생성하기 위해 이전 섹션에서 제안된 바로 그 접두사를 사용하고 있습니다. 실험은 8개의 NVIDIA Tesla V100 GPU에서 반정밀도(half-precision)를 사용하여 수행됩니다.

---
⁷그렇지 않으면 모델은 "도와드릴 수 없습니다."와 같이 항상 응답하는 사소한 행동으로 빠져들 수 있습니다.
⁸확률은 0에 가깝게 억제될 수도 있습니다.
⁹이 접두사는 안전성을 높이기 위해 오픈 소스 LLM[13, 49]에서 사용되는 시스템 프롬프트와 유사점을 공유합니다.

---

즉, Float16입니다. 부록에서는 대체 훈련 패러다임인 저순위 적응(LoRA) [54]에 대한 결과도 보여줍니다(부록 D.1 참조). 커뮤니티 표준[3, 1, 2]에 따라, 모델 평가에는 탐욕적 디코딩, 즉 `T = 0`을 사용합니다.

먼저, 제안된 DPO 수정안인 E-DPO가 실제로 Harmbench 데이터셋[2]과 AdvBench 유해 행동 데이터셋[1]의 첫 100개 프롬프트를 사용하여 HarmBench 프로토콜로 측정한 안전성 정렬을 개선한다는 경험적 증거를 보여줍니다. 모든 적에 대한 개요는 부록 F에 제공합니다. 결과는 표 1에 제시되어 있습니다. E-DPO는 우리가 테스트한 모든 작업에서 개선을 달성합니다.

안전성 결과 외에도, 우리는 E-RLHF가 증가된 안전성을 위해 유용성을 희생하지 않도록 하고 싶습니다. 우리는 MT-Bench 프로젝트[3]로 유용성을 평가합니다. SFT 모델 `PSFT`는 6.3점을 받았고, DPO와 E-DPO 모델 모두 그보다 더 나은 성능을 보였습니다(각각 6.9와 6.7). 이는 성능 저하가 우리 제안의 문제가 아님을 믿게 합니다. 다음으로, 우리는 모델 성능에 대한 안전 접두사의 영향을 보여줍니다. 우리는 우리 방법의 성능이 안전 접두사의 선택에 어느 정도 의존하지만 결코 실패하지 않음을 보여줍니다(부록 D.2 참조). 우리는 명시적인 튜닝을 통해 더 나은 안전 접두사를 찾는 것이 Yang et al. [55]의 연구와 유사하게 결과를 개선할 것이라고 믿지만, 이 탐색은 향후 연구를 위해 남겨둡니다. 또한, 우리는 개선이 해로운 프롬프트에 대한 KL 항에서 안전한 사전을 사용함으로써 발생한다는 것을 확인합니다. 우리는 선호도 정렬 데이터셋의 모든 프롬프트에 접두사를 추가하여 결과를 절제합니다(부록 D.3 참조). 모든 경우에, 일반 프롬프트에 안전 접두사를 적용하면 안전성이 저하되어, 해로운 프롬프트에만 사전을 전환하는 것의 중요성을 보여줍니다. 마지막으로, E-DPO가 모든 시스템 프롬프트와 결합하여 안전성을 더욱 높일 수 있음을 보여줍니다(부록 D.4 참조). 이 제안은 유용성과 안전성을 동시에 개선하는 데 사용될 수도 있습니다(부록 D.5 참조).

## 7 Conclusion and discussions

이 논문에서는 입력 프롬프트를 쿼리와 개념 쌍으로 분해하여 언어 모델 사전 훈련 및 탈옥을 위한 이론적 프레임워크를 제시합니다. 이 접근 방식을 통해, 우리는 사전 훈련 후 세계를 모방하는 언어 모델의 능력에 관한 두 가지 이론적 결과를 확립했으며, 이는 해로운 프롬프트가 주어졌을 때 해로운 설명을 출력하는 것으로 이어집니다. 그리고 정렬 문제로 인한 탈옥의 불가피성입니다. 이러한 이론적 통찰력에 따라, 우리는 안전성 정렬을 향상시키기 위한 간단하면서도 효과적인 기술을 고안했으며, 이 방법론으로 탈옥 공격에 대한 향상된 복원력을 입증했습니다.

현재의 한계 (1) 개념을 해롭거나 해롭지 않은 것으로 분류했지만, 개념의 잠재적 해악에 대한 인식은 상황을 집합적으로 형성하는 문화적, 법적, 사회적 규범과 같은 다양한 요인에 의해 영향을 받을 수 있음을 인정하는 것이 중요합니다. (2) 언어 모델은 다중 라운드, 다단계 대화 내에서 추론하고 작업을 완료하는 인상적인 능력을 보여주었습니다. 우리의 현재 프레임워크는 이러한 입력 형식과 관련된 일반화 및 탈옥 가능성을 완전히 설명하지 못할 수 있습니다. (3) 우리의 분석은 고정된 `pworld` 매핑 및 `Dp` 분포에 기반합니다. 그럼에도 불구하고, `pworld`와 `Dp`가 지속적으로 진화함에 따라 세계는 본질적으로 역동적입니다.

**Future work** (1) 실험 섹션에서 강조된 바와 같이, 우리의 E-RLHF 접근 방식과 관련하여, 모든 해로운 프롬프트에 보편적으로 안전한 접두사를 붙이는 것 외에도, 해로운 프롬프트를 개별적으로 변환함으로써 개선을 이룰 수 있습니다. 또한, 안전 변환된 프롬프트는 기존의 RLHF를 위한 선호도 데이터셋을 확장하는 데 사용될 수 있습니다. (2) 분석 전반에 걸쳐, 우리는 언어 모델의 용량에 어떠한 제약도 가하지 않았습니다. 유한 메모리 제약 하에서 분석을 확장하거나 LLM의 환각 속성을 분석하는 것은 탐구할 흥미로운 방향입니다. (3) 대규모 언어 모델은 문맥 내 학습자[56]로서 놀라운 능력을 보여주었으며, 이러한 기술은 잠재적으로 그들을 탈옥시키는 데에도 사용될 수 있습니다[57-59]. 이러한 입력 패러다임의 통합을 조사하는 것은 미래 연구를 위한 유망한 길로 남아 있습니다.

**Acknowledgement**
JS와 JK는 국립 과학 재단(National Science Foundation)의 NSF Award 1922658에 의해 지원받았습니다. 이 연구의 일부는 JK가 2023/24년에 École Normale Supérieure(ENS)의 Centre Sciences de Donnees에 초청되었을 때 수행되었으며, 그녀의 환대에 깊이 감사합니다.

## References
- [1] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023. 1, 2, 9, 27, 28, 29, 30, 31, 32
- [2] Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, et al. Harmbench: A standardized evaluation framework for automated red teaming and robust refusal. arXiv preprint arXiv:2402.04249, 2024. 1, 2, 9, 27, 28, 29
- [3] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging Ilm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 37, 2023. 1, 9,28
- [4] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. 1
- [5] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature, 620(7972):172–180, 2023. 1
- [6] Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan J Prenger, and Animashree Anandkumar. Leandojo: Theorem proving with retrieval-augmented language models. Advances in Neural Information Processing Systems, 36, 2024. 1
- [7] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 610–623, 2021. 1
- [8] Julian Hazell. Large language models can be used to effectively scale spear phishing campaigns. arXiv preprint arXiv:2305.06972, 2023.
- [9] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. arXiv preprint arXiv:2305.13860, 2023. 1, 30
- [10] OpenAI. Chatgpt. https://openai.com/index/chatgpt/, 2022. 1
- [11] DAN. Do anything now prompt. https://github.com/0xk1h0/ChatGPT_DAN, 2023. 1
- [12] Reddit. Chatgpt grandma exploit. https://www.reddit.com/r/ChatGPT/comments/12sn0kk/grandma_exploit/, 2023. 1
- [13] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2, 8, 22, 26
- [14] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022. 2, 31
- [15] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. arXiv preprint arXiv:2202.03286, 2022. 29
- [16] Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan Hadfield-Menell. Explore, establish, exploit: Red teaming language models from scratch. arXiv preprint arXiv:2306.09442, 2023.
- [17] Zhang-Wei Hong, Idan Shenfeld, Tsun-Hsuan Wang, Yung-Sung Chuang, Aldo Pareja, James R Glass, Akash Srivastava, and Pulkit Agrawal. Curiosity-driven red-teaming for large language models. In The Twelfth International Conference on Learning Representations, 2023.
- [18] Mikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, et al. Rainbow teaming: Open-ended generation of diverse adversarial prompts. arXiv preprint arXiv:2402.16822, 2024. 2, 31
- [19] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 2, 25, 31
- [20] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022. 25, 31
- [21] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. 25, 31
- [22] Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher Buckley, Jason Phang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human preferences. In International Conference on Machine Learning, pages 17506–17533. PMLR, 2023. 31
- [23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023. 2, 3, 7, 25, 26, 33
- [24] Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun. Autodan: Automatic and interpretable adversarial attacks on large language models. arXiv preprint arXiv:2310.15140, 2023. 2, 30
- [25] Raz Lapid, Ron Langberg, and Moshe Sipper. Open sesame! universal black box jailbreaking of large language models. arXiv preprint arXiv:2309.01446, 2023. 2, 30
- [26] Zheng-Xin Yong, Cristina Menghini, and Stephen H Bach. Low-resource languages jailbreak gpt-4. arXiv preprint arXiv:2310.02446, 2023. 2, 31
- [27] Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing. Multilingual jailbreak challenges in large language models. arXiv preprint arXiv:2310.06474, 2023. 31
- [28] Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher. arXiv preprint arXiv:2308.06463, 2023. 2, 31
- [29] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451, 2023. 2, 29, 30
- [30] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419, 2023. 28, 29, 30
- [31] Xiaoxia Li, Siyuan Liang, Jiyi Zhang, Han Fang, Aishan Liu, and Ee-Chien Chang. Semantic mirror jailbreak: Genetic algorithm based jailbreak prompts against open-source llms. arXiv preprint arXiv:2402.14872, 2024. 30
- [32] Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen, and Shujian Huang. A wolf in sheep's clothing: Generalized nested jailbreak prompts can fool large language models easily. arXiv preprint arXiv:2311.08268, 2023. 30
- [33] Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, and Amin Karbasi. Tree of attacks: Jailbreaking black-box llms automatically. arXiv preprint arXiv:2312.02119, 2023. 2, 29, 30
- [34] Alexander Robey, Eric Wong, Hamed Hassani, and George J Pappas. Smoothllm: Defending large language models against jailbreaking attacks. arXiv preprint arXiv:2310.03684, 2023. 2, 32, 33
- [35] Bochuan Cao, Yuanpu Cao, Lu Lin, and Jinghui Chen. Defending against alignment-breaking attacks via robustly aligned Ilm. arXiv preprint arXiv:2309.14348, 2023. 32
- [36] Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, and Hima Lakkaraju. Certifying Ilm safety against adversarial prompting. arXiv preprint arXiv:2309.02705, 2023. 32
- [37] Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. Baseline defenses for adversarial attacks against aligned language models. arXiv preprint arXiv:2309.00614, 2023. 2, 32
- [38] Yotam Wolf, Noam Wies, Yoav Levine, and Amnon Shashua. Fundamental limitations of alignment in large language models. arXiv preprint arXiv:2304.11082, 2023. 2, 3, 33
- [39] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. 2
- [40] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. In International Conference on Learning Representations, 2018. 2
- [41] Ali Shafahi, W Ronny Huang, Christoph Studer, Soheil Feizi, and Tom Goldstein. Are adversarial examples inevitable? In International Conference on Learning Representations, 2018.5
- [42] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In international conference on machine learning, pages 1310–1320. PMLR, 2019. 2
- [43] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. 4
- [44] David A McAllester. Some pac-bayesian theorems. In Proceedings of the eleventh annual conference on Computational learning theory, pages 230–234, 1998. 4
- [45] Sokhna Diarra Mbacke, Florence Clerc, and Pascal Germain. Pac-bayesian generalization bounds for adversarial generative models. arXiv preprint arXiv:2302.08942, 2023. 5
- [46] Pascal Germain, Alexandre Lacasse, François Laviolette, and Mario Marchand. Pac-bayesian learning of linear classifiers. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 353-360, 2009. 5
- [47] Maxime Haddouche, Benjamin Guedj, Omar Rivasplata, and John Shawe-Taylor. Pac-bayes unleashed: Generalisation bounds with unbounded losses. Entropy, 23(10):1330, 2021. 5
- [48] Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. Catastrophic jailbreak of open-source llms via exploiting generation. arXiv preprint arXiv:2310.06987, 2023. 6, 27, 30
- [49] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 8
- [50] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Shengyi Huang, Kashif Rasul, Alexander M. Rush, and Thomas Wolf. The alignment handbook. https://github.com/huggingface/alignment-handbook, 2023. 8
- [51] HuggingFace. Huggingface constitutional-ai sft model. https://huggingface.co/alignment-handbook/mistral-7b-sft-constitutional-ai, 2024. 8
- [52] HuggingFace. Huggingface constitutional-ai dataset: Ultrafeedback-binarized. https://huggingface.co/datasets/HuggingFaceH4/ultrafeecback_binarized, 2024. 8
- [53] HuggingFace. Huggingface constitutional-ai dataset: Cai-conversation-harmless. https://huggingface.co/datasets/HuggingFaceH4/cai-conversation-harmless, 2024. 8
- [54] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021. 9, 27
- [55] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023. 9, 26
- [56] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. 10
- [57] Zeming Wei, Yifei Wang, and Yisen Wang. Jailbreak and guard aligned language models with only few in-context demonstrations. arXiv preprint arXiv:2310.06387, 2023. 10, 31
- [58] Jiongxiao Wang, Zichen Liu, Keun Hee Park, Muhao Chen, and Chaowei Xiao. Adversarial demonstration attacks on large language models. arXiv preprint arXiv:2305.14950, 2023. 31
- [59] Cem Anil, Esin Durmus, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Batson, Nina Rimsky, Meg Tong, Jesse Mu, Daniel Ford, et al. Many-shot jailbreaking, 2024. 10, 31
- [60] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep learning. Advances in neural information processing systems, 30, 2017. 21
- [61] Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simple baseline for bayesian uncertainty in deep learning. Advances in neural information processing systems, 32, 2019. 21
- [62] Robert D Gordon. Values of mills' ratio of area to bounding ordinate and of the normal probability integral for large values of the argument. The Annals of Mathematical Statistics, 12(3):364-366, 1941. 23
- [63] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. 25
- [64] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324–345, 1952. 25
- [65] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 25
- [66] Banghua Zhu, Hiteshi Sharma, Felipe Vieira Frujeri, Shi Dong, Chenguang Zhu, Michael I Jordan, and Jiantao Jiao. Fine-tuning language models with advantage-induced policy alignment. arXiv preprint arXiv:2306.02231, 2023. 25
- [67] Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. Implementation matters in deep policy gradients: A case study on ppo and trpo. arXiv preprint arXiv:2005.12729, 2020. 25
- [68] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824–24837, 2022. 26
- [69] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. 26
- [70] Xiaotian Zou, Yongkang Chen, and Ke Li. Is the system message really important to jailbreaks in large language models? arXiv preprint arXiv:2402.14857, 2024. 27, 32
- [71] Chuan Guo, Alexandre Sablayrolles, Hervé Jégou, and Douwe Kiela. Gradient-based adversarial attacks against text transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5747–5757, 2021. 29, 30
- [72] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980, 2020. 29, 30
- [73] Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms. arXiv preprint arXiv:2401.06373, 2024. 29, 31
- [74] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. " do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. arXiv preprint arXiv:2308.03825, 2023. 29, 30
- [75] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial triggers for attacking and analyzing nlp. arXiv preprint arXiv:1908.07125, 2019. 29
- [76] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. Advances in Neural Information Processing Systems, 36, 2024. 29
- [77] Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt. Automatically auditing large language models via discrete optimization. arXiv preprint arXiv:2303.04381, 2023. 30
- [78] Fábio Perez and Ian Ribeiro. Ignore previous prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527, 2022. 30
- [79] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 30
- [80] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. Jailbreaker: Automated jailbreak across multiple large language model chatbots. arXiv preprint arXiv:2307.08715, 2023. 30
- [81] Jiahao Yu, Xingwei Lin, and Xinyu Xing. Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts. arXiv preprint arXiv:2309.10253, 2023. 30
- [82] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024. 30
- [83] Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. Deepinception: Hypnotize large language model to be jailbreaker. arXiv preprint arXiv:2311.03191, 2023. 30
- [84] Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. Drattack: Prompt decomposition and reconstruction makes powerful llm jailbreakers. arXiv preprint arXiv:2402.16914, 2024. 30
- [85] Zhenhua Wang, Wei Xie, Baosheng Wang, Enze Wang, Zhiwen Gui, Shuoyoucheng Ma, and Kai Chen. Foot in the door: Understanding large language model jailbreaking via cognitive psychology. arXiv preprint arXiv:2402.15690, 2024. 30
- [86] Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos, and Yuandong Tian. Advprompter: Fast adaptive adversarial prompting for llms. arXiv preprint arXiv:2404.16873, 2024. 30
- [87] Hangfan Zhang, Zhimeng Guo, Huaisheng Zhu, Bochuan Cao, Lu Lin, Jinyuan Jia, Jinghui Chen, and Dinghao Wu. On the safety of open-sourced large language models: Does alignment really prevent them from being misused? arXiv preprint arXiv:2310.01581, 2023. 30
- [88] Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, and William Yang Wang. Weak-to-strong jailbreaking on large language models. arXiv preprint arXiv:2401.17256, 2024. 30
- [89] Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, and Dahua Lin. Shadow alignment: The ease of subverting safely-aligned language models. arXiv preprint arXiv:2310.02949, 2023. 30
- [90] Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori Hashimoto, and Daniel Kang. Removing rlhf protections in gpt-4 via fine-tuning. arXiv preprint arXiv:2311.05553, 2023. 30
- [91] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693, 2023. 30
- [92] Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models. arXiv preprint arXiv:2307.14539, 2023. 31
- [93] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, and Prateek Mittal. Visual adversarial examples jailbreak large language models. arXiv preprint arXiv:2306.13213, 2023. 31
- [94] Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, Irena Gao, Pang Wei Koh, Daphne Ippolito, Florian Tramèr, and Ludwig Schmidt. Are aligned neural networks adversarially aligned? In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 31
- [95] Natalie Maus, Patrick Chao, Eric Wong, and Jacob R Gardner. Black box adversarial prompting for foundation models. In The Second Workshop on New Frontiers in Adversarial Machine Learning, 2023. 31
- [96] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/. 31
- [97] Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. Exploiting programmatic behavior of llms: Dual-use through standard security attacks. arXiv preprint arXiv:2302.05733, 2023. 31
- [98] Rusheb Shah, Soroush Pour, Arush Tagade, Stephen Casper, Javier Rando, et al. Scalable and transferable black-box jailbreaks for language models via persona modulation. arXiv preprint arXiv:2311.03348, 2023. 31
- [99] Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, and Bin Hu. Cold-attack: Jailbreaking llms with stealthiness and controllability. arXiv preprint arXiv:2402.08679, 2024. 31
- [100] Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. Cold decoding: Energy-based constrained text generation with langevin dynamics. Advances in Neural Information Processing Systems, 35:9538–9551, 2022. 31
- [101] Somnath Banerjee, Sayan Layek, Rima Hazra, and Animesh Mukherjee. How (un) ethical are instruction-centric responses of llms? unveiling the vulnerabilities of safety guardrails to harmful queries. arXiv preprint arXiv:2402.15302, 2024. 31
- [102] Neal Mangaokar, Ashish Hooda, Jihye Choi, Shreyas Chandrashekaran, Kassem Fawaz, Somesh Jha, and Atul Prakash. Prp: Propagating universal perturbations to attack large language model guard-rails. arXiv preprint arXiv:2402.15911, 2024. 31
- [103] Huijie Lv, Xiao Wang, Yuansen Zhang, Caishuang Huang, Shihan Dou, Junjie Ye, Tao Gui, Qi Zhang, and Xuanjing Huang. Codechameleon: Personalized encryption framework for jailbreaking large language models. arXiv preprint arXiv:2402.16717, 2024. 31
- [104] Vinu Sankar Sadasivan, Shoumik Saha, Gaurang Sriramanan, Priyatham Kattakinda, Atoosa Chegini, and Soheil Feizi. Fast adversarial attacks on language models in one gpu minute. arXiv preprint arXiv:2402.15570, 2024. 31
- [105] Jonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah, Yuxin Wen, and Tom Goldstein. Coercing llms to do and reveal (almost) anything. arXiv preprint arXiv:2402.14020, 2024. 31
- [106] Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Wai Lam, and Lizhuang Ma. Exploring safety generalization challenges of large language models via code, 2024. 31
- [107] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018. 31
- [108] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308–318, 2016. 31
- [109] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. 31
- [110] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023. 31
- [111] Gabriel Alon and Michael Kamfonas. Detecting language model attacks with perplexity. arXiv preprint arXiv:2308.14132, 2023. 32
- [112] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30, 2017. 32
- [113] Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, Xing Xie, and Fangzhao Wu. Defending chatgpt against jailbreak attack via self-reminders. Nature Machine Intelligence, pages 1–11, 2023. 32
- [114] Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, and Nanyun Peng. Prompt-driven Ilm safeguarding via directed representation optimization. arXiv preprint arXiv:2401.18018, 2024. 32
- [115] Andy Zhou, Bo Li, and Haohan Wang. Robust prompt optimization for defending language models against jailbreaking attacks. arXiv preprint arXiv:2401.17263, 2024. 32
- [116] Yichuan Mo, Yuji Wang, Zeming Wei, and Yisen Wang. Studious bob fight back against jailbreaking via prompt adversarial tuning. arXiv preprint arXiv:2402.06255, 2024. 32
- [117] Zhexin Zhang, Junxiao Yang, Pei Ke, and Minlie Huang. Defending large language models against jailbreaking attacks through goal prioritization. arXiv preprint arXiv:2311.09096, 2023.32
- [118] Yujun Zhou, Yufei Han, Haomin Zhuang, Taicheng Guo, Kehan Guo, Zhenwen Liang, Hongyan Bao, and Xiangliang Zhang. Defending jailbreak prompts via in-context adversarial game. arXiv preprint arXiv:2402.13148, 2024. 32
- [119] Alec Helbling, Mansi Phute, Matthew Hull, and Duen Horng Chau. Llm self defense: By self examination, Ilms know they are being tricked. arXiv preprint arXiv:2308.07308, 2023. 32
- [120] Zezhong Wang, Fangkai Yang, Lu Wang, Pu Zhao, Hongru Wang, Liang Chen, Qingwei Lin, and Kam-Fai Wong. Self-guard: Empower the Ilm to safeguard itself. arXiv preprint arXiv:2310.15851, 2023. 32
- [121] Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, and Hongyang Zhang. Rain: Your language models can align themselves without finetuning. arXiv preprint arXiv:2309.07124, 2023. 32
- [122] Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill Yuchen Lin, and Radha Poovendran. Safedecoding: Defending against jailbreak attacks via safety-aware decoding. arXiv preprint arXiv:2402.08983, 2024. 32
- [123] Adib Hasan, Ileana Rugina, and Alex Wang. Pruning for protection: Increasing jailbreak resistance in aligned llms without fine-tuning. arXiv preprint arXiv:2401.10862, 2024. 32
- [124] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023. 32
- [125] Renjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, and Tong Zhang. Mllm-protector: Ensuring mllm's safety without hurting performance. arXiv preprint arXiv:2401.02906, 2024. 32
- [126] Yuqi Zhang, Liang Ding, Lefei Zhang, and Dacheng Tao. Intention analysis prompting makes large language models a good jailbreak defender. arXiv preprint arXiv:2401.06561, 2024. 32
- [127] Yihan Wang, Zhouxing Shi, Andrew Bai, and Cho-Jui Hsieh. Defending Ilms against jailbreaking attacks via backtranslation. arXiv preprint arXiv:2402.16459, 2024. 32
- [128] Heegyu Kim, Sehyun Yuk, and Hyunsouk Cho. Break the breakout: Reinventing Im defense against jailbreak attacks with self-refinement. arXiv preprint arXiv:2402.15180, 2024. 32
- [129] Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, and Qingyun Wu. Autodefense: Multi-agent Ilm defense against jailbreak attacks, 2024. 32
- [130] Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho. Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes. arXiv preprint arXiv:2403.00867, 2024. 33
- [131] Jiabao Ji, Bairu Hou, Alexander Robey, George J Pappas, Hamed Hassani, Yang Zhang, Eric Wong, and Shiyu Chang. Defending large language models against jailbreak attacks via semantic smoothing. arXiv preprint arXiv:2402.16192, 2024. 33
- [132] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et al. Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.066742023. 33
- [133] Swapnaja Achintalwar, Adriana Alvarado Garcia, Ateret Anaby-Tavor, Ioana Baldini, Sara E. Berger, Bishwaranjan Bhattacharjee, Djallel Bouneffouf, Subhajit Chaudhury, Pin-Yu Chen, Lamogha Chiazor, Elizabeth M. Daly, Rogério Abreu de Paula, Pierre Dognin, Eitan Farchi, Soumya Ghosh, Michael Hind, Raya Horesh, George Kour, Ja Young Lee, Erik Miehling, Keerthiram Murugesan, Manish Nagireddy, Inkit Padhi, David Piorkowski, Ambrish Rawat, Orna Raz, Prasanna Sattigeri, Hendrik Strobelt, Sarathkrishna Swaminathan, Christoph Tillmann, Aashka Trivedi, Kush R. Varshney, Dennis Wei, Shalisha Witherspooon, and Marcel Zalmanovici. Detectors for safe and reliable llms: Implementations, uses, and limitations, 2024. 33
- [134] Adam Tauman Kalai and Santosh S Vempala. Calibrated language models must hallucinate. arXiv preprint arXiv:2311.14648, 2023. 33
- [135] Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K Kummerfeld, and Rada Mihalcea. A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity. arXiv preprint arXiv:2401.01967, 2024. 33
- [136] Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, and Peter Henderson. Assessing the brittleness of safety alignment via pruning and low-rank modifications. arXiv preprint arXiv:2402.05162, 2024. 33

## Appendix

## A Glossary

**Table 2: Summary of notation.**
(표 내용은 가이드라인에 따라 생략합니다.)

## B Proof of Theorems

### B.1 Proof of PAC-Bayesian bounds

**Definition B.1.** (Bounded Difference) 함수 $f : \mathcal{X}^n \to \mathbb{R}$는 상수 $c_1, ..., c_n$의 집합에 대해 경계 차이 속성을 갖는다고 하며, 다음을 만족합니다:
$ \sup_{x_1, ..., x_n, x_i'} |f(x_1,...,x_n) - f (x_1, ..., x_{i-1}, x_i', ..., x_n)| \le c_i, \forall i \in [n] $

**Lemma B.1.** (Hoeffding's Lemma) 확률 1로 $X \in [a, b]$인 확률 변수 $X$에 대해 다음이 성립합니다:
$ E[\exp(\lambda X)] \le \exp(\lambda EX + \frac{\lambda^2(b-a)^2}{8}) $

**Lemma B.2.** (Hoeffding's Lemma, Multivariate) $f$가 경계 차이 속성을 갖는 확률 변수 $Z = f(x_1, ..., x_n)$에 대해 다음이 성립합니다:
$ E[\exp(\lambda(EZ – Z))] \le \exp(\frac{\lambda^2 \sum c_i^2}{8}) $

$Z$를 $R_S(LM)$으로 대체하는 것이 유효합니다.

**Lemma B.3.** 정의 3.1에 정의된 경험적 손실은 상수 $c_i = 1/n, \forall i$로 경계 차이 조건을 만족합니다.

이제 정리 1의 증명을 제시할 준비가 되었습니다.

*Proof.* 위의 보조정리에서 시작하여, 우리는 다음을 압니다.
$ E_S[\exp(\lambda(R(LM) – R_S(LM)))] \le \exp(\frac{\lambda^2}{8n}) $

위 결과는 수동으로 선택된 LM에 대해 성립합니다. 사전 분포 $\pi$에 대한 전체 평균을 사용하면 다음과 같습니다.
$ E_{LM \sim \pi} E_S[\exp(\lambda(R(LM) – R_S(LM)))] \le \exp(\frac{\lambda^2}{8n}) $

푸비니의 정리를 적용합니다($\pi$는 $S$와 독립적임에 유의):
$ E_S E_{LM \sim \pi}[\exp(\lambda(R(LM) – R_S(LM)))] \le \exp(\frac{\lambda^2}{8n}) $

### B.2 An estimation on the non-vacuousness of the PAC bound

우리는 PAC 경계에 나타나는 항 $e$에 대한 추정치를 제공하고, 그것이 비-자명함을 명시합니다.

**The numerator.** 우리는 Neyshabur et al. [60]을 따라 가장 간단한 설정에서 항을 인스턴스화합니다. $\pi$, $\rho$가 $K$개의 매개변수를 가진 주어진 LM의 매개변수 공간에 대해 정의되었다고 가정합니다. $w$가 사전 훈련 코퍼스에서 학습된 가중치 집합이라고 가정합니다. 사전 분포 $\pi$를 영-평균 다변량 가우시안으로, 그 항목별 분산이 가중치의 크기와 관련이 있다고 가정합니다: $\sigma_i = \beta|w_i|$, 그리고 $\rho$를 $w$를 중심으로 하는 동일한 이방성 분산을 가진 가우시안으로 가정합니다. 우리는 두 설정 모두 간단하지만 실용적이라고 주장합니다. 가우시안 초기화는 모델 훈련에 일반적이며, SWA-가우시안 알고리즘[61]은 이러한 가우시안 사후 분포를 활용하기 때문입니다. 이 설정 하에서, KL은 $\sum w_i^2/2\sigma_i^2 = O(K)$가 됩니다. 구체적으로, $\beta = \sqrt{2}$를 취하면 항이 정확히 $K$가 됩니다. 현재 언어 모델은 종종 수백만 또는 수십억 개의 매개변수를 가지고 있습니다. 즉, $K \sim [10^6, 10^9]$입니다.

**The denominator.** 훈련 코퍼스에서 고유한 직접 프롬프트의 수를 추정하기 위해, 데이터셋이 $(q, c)$ 프롬프트뿐만 아니라 $e$ 설명으로도 구성된다는 점에 유의하는 것이 중요합니다. 따라서, 우리는 각 고유 프롬프트 $x = (q, c)$와 관련된 평균 토큰 길이(ATL)를 추정해야 합니다. 각 고유 프롬프트 $x$에 대해, 자체 토큰 길이 $l(x)$ 외에도, 각 관련 설명의 예상 토큰 길이 $l(e)$를 가진 설명 모음 $\{e_i\}$가 있을 것입니다. 우리는 다음을 가집니다.
$ E_{ATL} = E_{x \sim D_p} N(x) \times [l(x) + l(e)] $

**a constant.** LLaMa-2 보고서(섹션 4.1, 그림 13) [13]에 따르면, 훈련 코퍼스의 문서 중 약 0.2%가 유해한 것으로 레이블링되었습니다. 그러나, 우리는 이것이 실제로 $\alpha$에 대한 극도로 느슨한 하한이라고 주장합니다. 그들의 논문에서 사용된 추정 전략 때문입니다. 문서가 주어지면, 그들은 각 단일 라인에 대해 유해성에 대한 이진 분류기(1은 유해, 0은 그렇지 않음)를 사용하고, 문서에 평균 점수를 할당합니다. 0.2%는 점수 $\ge 0.5$인 문서의 비율입니다. "폭탄 만드는 법"의 예를 들어보겠습니다. 화학 반응 부분은 유해한 것으로 계산되지 않을 것이므로, 이 추정 전략은 완전히 유해한 설명을 무해한 것으로 판단할 수 있습니다. 따라서, $\alpha$가 너무 작지 않다고 주장하는 것이 합리적이지만, 현재 문헌으로는 정확한 추정치를 제시할 수 없습니다.

### B.3 Proof of jailbreaking

증명을 진행하기 전에, 필요한 정의와 보조정리를 다음과 같이 나열합니다.
(증명 내용은 수학적이고 기술적이므로, 주요 보조정리 및 정의의 제목만 번역합니다.)

**Lemma B.4.** (Volume of n-simplex)
**Definition B.2.** (Projected probability simplex)
**Lemma B.5.** (Transformation of probability simplex)
**Lemma B.6.** (Gaussian cdf Tail Bound, Gordon [62])

### C RLHF, DPO and our E-RLHF

고전적인 RLHF 프레임워크는 Christiano et al. [63]에 의해 확립되었고, Ziegler et al. [19], Ouyang et al. [20], Bai et al. [21]에 의해 개발되었습니다. 선호도 데이터셋 $D_s = \{(x, e_w, e_l)\}$를 수집한 후, 먼저 Bradley-Terry 모델[64] 하에서 보상 모델을 훈련하며, 목표는 다음과 같습니다. 여기서 $\sigma(\cdot)$는 시그모이드 함수를 나타냅니다.
$ r(x, e) = \arg \max_r E_{(x,e_w,e_l) \sim D_s} \log \sigma(r(x, e_w) – r(x, e_l)) $

다음으로, 근접 정책 최적화(PPO) [65]가 이러한 구현 전반에 걸쳐 공통적으로 채택되어 현재 최첨단 언어 모델의 기초를 형성합니다. KL-제약 RL 미세 조정(RLFT) 목표는 다음과 같은 형태를 취합니다.
$ \max_{P_{LM}} E_{x \sim D_s} \{E_{e \sim p_{LM}(\cdot|x)}[r(x, e)] – \beta D_{KL}(P_{LM}(x)||P_{SFT}(x))\} $

그러나 PPO 튜닝은 불안정성[66]과 구현 복잡성[67]으로 어려움을 겪을 수 있습니다. 이러한 문제를 극복하기 위해, 일련의 연구에서는 보상 모델링 단계를 건너뛰고 선호도 데이터셋에서 직접 학습할 것을 제안하며, 대표적인 선구적 연구는 Rafailov et al. [23]의 직접 선호도 최적화(DPO)입니다. 우리는 아래에서 DPO 목표의 유도를 요약하고, 우리 실험에서 사용하는 목표, 즉 E-DPO로 일반화합니다.

### D Ablation Study

이 섹션에서는 제안된 E-RLHF의 효과를 보여주기 위해 광범위한 절제 연구를 수행합니다.

**D.1 LORA results**
이 섹션에서는 저순위 적응(Low-Rank Adaptation) [54]으로 얻은 결과를 보여줍니다. 우리는 절제 D.2에서와 동일한 안전 접두사 집합을 탐색하고, 설명을 위해 최상의 모델을 선택합니다. 숫자는 표 3에 설명되어 있습니다. 결과는 전체 매개변수 튜닝을 통해 얻은 것과 동일하며, 우리의 E-RLHF가 RLHF 기준선에 대해 일관되게 더 나은 성능을 보인다는 것입니다.

**D.2 Ablation on safe prefixes**
우리는 다른 안전 접두사의 효과를 절제합니다. 우리가 고려하는 접두사는 표 8에 수집되어 있습니다. 공격 성공률 숫자는 표 4에 나와 있으며, 각 모델의 MT-Bench 점수는 괄호 안에 표시되어 있습니다. 분명히, 거의 모든 안전 접두사는 DPO 기준선에 비해 더 나은 안전성을 제공하고 SFT 모델(MT-Bench 점수 6.3)에 비해 향상된 유용성을 제공하며, 성능은 접두사 선택에 따라 달라질 수 있습니다. 좋은 접두사를 찾는 것은 우리 방법에서 중요하며, 최적의 것을 찾는 것은 향후 연구로 남겨둡니다.

**D.3 Ablation on transforming all prompts**
여기서는 모든 프롬프트를 변환하는 효과를 절제합니다. 유해 여부에 관계없이. 결과는 표 5에 나와 있으며, 빨간색은 유해한 프롬프트만 변환하여 얻은 모델에 비해 안전성이 저하되었음을 나타냅니다. 분명히, 대부분의 모델은 DPO 기준선 자체에 비해 더 나쁜 안전성을 유지하며, 이는 무해한 프롬프트를 변환하는 해로운 효과를 시사합니다.

**D.4 Ablation with system prompt**
이전 연구[2, 48, 70]에서 지적했듯이, 시스템 프롬프트는 ASR에 상당한 영향을 미칠 수 있습니다. 이는 두 가지 측면에서 비롯됩니다: 첫째, 강력한 시스템 프롬프트는 LM을 안전 구역에 더 가깝게 초기화하여 모델을 더 안전하게 만들 수 있습니다. 둘째, 더 긴 시스템 프롬프트는 증가된 계산 소비로 인해 특정 공격을 시작하는 어려움을 확대할 것입니다. 이 특정 상황에서 E-RLHF가 여전히 유용한지 확인하기 위해, 우리는 기본 Mistral 시스템 프롬프트의 제어 하에서 이 효과를 절제합니다. 결과는 표 6에서 찾을 수 있으며, 거의 모든 ASR이 떨어지지만 결론은 바뀌지 않습니다.

**D.5 Ablation on improving helpfulness and safety simultaneously**
비슷한 정신으로, 우리는 안전성을 향상시키기 위해 제안하는 트릭이 유용성을 동시에 향상시키는 데 사용될 수 있는지 묻습니다. 이 목표를 달성하기 위해, 우리는 "다음 요청에 대해 안전하고 유용한 응답을 제공하십시오."라는 접두사를 선택하고, 선호도 데이터셋의 모든 프롬프트에 적용합니다. 결과는 표 7에서 찾을 수 있습니다. 우리 모델은 더 나은 안전성을 달성하고, MT-Bench[3]로 벤치마킹된 유용성을 6.9에서 7.0으로 향상시킵니다.

### E Filtering harmful prompt in the preference alignment dataset

Chao et al. [30]의 연구에 영감을 받아, 우리는 입력 프롬프트에 대한 유해성 판단자로서 GPT-3.5-Turbo에 프롬프트를 보냅니다. 이는 주어진 프롬프트에 대해 1에서 10까지의 정수 점수를 할당하며, 우리는 점수 > 6인 부분집합을 유해한 것으로 선택합니다. 시스템 프롬프트는 표 9에 나와 있습니다.

### F Jailbreak adversaries collected in the HarmBench project [2]

이 섹션에서는 모델을 평가하기 위해 채택한 적들에 대한 간략한 개요를 제공합니다. 일부 설명은 Mazeika et al. [2]에 요약되어 있습니다.

- **Direct Request**는 원래의 유해한 프롬프트를 대상 LLM에 직접 보내는 것을 의미합니다.
- **GCG** [1], **GBDA** [71] 및 **AP** [72]는 토큰 수준 최적화를 통해 적대적 접미사를 찾습니다.
- **SFS** (Stochastic Few-Shot) 및 **ZS** (Zero-Shot) [15]는 공격자 LLM에 의한 테스트 케이스의 few-shot 샘플링 또는 zero-shot 생성으로 대상 LLM에서 행동을 유도합니다.
- **PAIR** [30] 및 **TAP** [33]은 공격자 LLM을 사용하여 대상 LLM에서 특정 유해한 행동을 적응적으로 탐색하고 유도하는 반복적 프롬프팅 / 트리 구조 프롬프팅 방법입니다.
- **AutoDAN** [29]은 수작업으로 만든 DAN 탈옥 프롬프트에서 초기화된 유전 알고리즘 기반 공격입니다.
- **PAP** [73]는 대상 LLM을 탈옥시키기 위해 설득력 있는 전략을 사용합니다.
- **HumanJailbreaks** [74]는 Do Anything Now (DAN) 탈옥과 유사한, 실제 인간 탈옥 템플릿의 고정된 집합을 사용합니다.

우리는 단일 모델 탈옥에 초점을 맞추기 때문에 모든 전이 공격을 제외합니다. 또한, UAT [75] 및 PEZ [76] 적을 버리기로 선택했는데, 전자는 V100 GPU에서 메모리 부족 오류를 유발하고 후자는 우리 실험에 따르면 접미사를 찾는 데 결코 성공하지 못했기 때문입니다.

### G Broader Impacts

우리 연구의 사회적 영향은 LLM 안전성 주제와 밀접한 관련이 있습니다. 우리는 언어 모델 사전 훈련 및 탈옥을 분석하기 위한 프레임워크를 제안하고, 안전성 향상을 위한 새로운 RLHF 알고리즘을 설계합니다. 실험에서 보여주듯이, 우리 연구는 더 안전한 LLM을 옹호할 수 있습니다.

### H Related work

이 섹션에서는 LLM 탈옥에 대한 현재 문헌을 검토합니다.

**H.1 Jailbreak methods**
이 섹션에서는 기존의 탈옥 방법들을 요약합니다.

**H.2 Defense methods**
지금까지, 적대적 공격에 대한 적대적 훈련[107] / 차등 프라이버시[108]와 같은 보편적인 방어 전략은 존재하지 않습니다. 일반적으로, 우리는 방법을 세 가지 유형으로 분류할 수 있습니다: 정렬, 레드팀, 그리고 알고리즘 제안.

**H.3 Theory and experimental understanding**
Wolf et al. [38]은 LLM 출력을 좋은 구성 요소와 나쁜 구성 요소로 분해할 수 있다고 가정하고, 충분히 긴 입력으로 모델에 프롬프트를 보내 이론적으로 가능한 탈옥을 보여줍니다. Kalai와 Vempala [134]는 보정된 LM에 대한 환각을 증명하기 위해 통계적 도구를 사용합니다. Lee et al. [135]는 GPT-2의 표현을 연구합니다. 그들은 독성에 대한 기본 분류기를 훈련하고, 선형 가중치를 독성 벡터의 프록시로 사용합니다. 그들은 DPO 튜닝[23]에 의해 억제되지 않는 독성 벡터 자체에 가까운 값 벡터가 있음을 발견합니다. Wei et al. [136]은 안전한 LM에 대한 가지치기 및 저순위 분석을 사용하고, (1) 안전한 뉴런과 유용한 뉴런은 희소하다는 것을 발견합니다. 안전한 뉴런을 가지치기하거나 안전한 순위를 제거하면 안전성이 많이 저하되며, (2) 미세 조정에서 안전한 뉴런을 고정하는 것은 안전성을 유지하지 못합니다.
