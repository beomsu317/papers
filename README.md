# Attention Is All You Need

## 초록

지배적인 시퀀스 변환 모델은 인코더와 디코더를 포함하는 복잡한 순환 또는 컨볼루션 신경망을 기반으로 합니다. 최고 성능의 모델은 어텐션 메커니즘을 통해 인코더와 디코더를 연결합니다. 우리는 순환과 컨볼루션을 완전히 없애고 전적으로 어텐션 메커니즘에 기반한 새로운 간단한 네트워크 아키텍처인 트랜스포머를 제안합니다. 두 가지 기계 번역 작업에 대한 실험은 이러한 모델이 품질이 우수하면서도 병렬화가 더 용이하고 훈련 시간이 훨씬 적게 소요된다는 것을 보여줍니다. 우리 모델은 WMT 2014 영어-독일어 번역 작업에서 28.4 BLEU를 달성하여 앙상블을 포함한 기존 최고 결과를 2 BLEU 이상 향상시켰습니다. WMT 2014 영어-프랑스어 번역 작업에서 우리 모델은 8개의 GPU에서 3.5일 동안 훈련한 후 41.8의 새로운 단일 모델 최첨단 BLEU 점수를 수립했으며, 이는 문헌의 최고 모델 훈련 비용의 일부에 불과합니다. 우리는 트랜스포머가 크고 제한된 훈련 데이터를 사용하여 영어 구문 분석에 성공적으로 적용함으로써 다른 작업에도 잘 일반화됨을 보여줍니다.

## 1. 소개

특히 순환 신경망, 장단기 기억(LSTM) 및 게이트 순환 신경망은 언어 모델링 및 기계 번역과 같은 시퀀스 모델링 및 변환 문제에서 최첨단 접근 방식으로 확고히 자리 잡았습니다. 이후 수많은 노력을 통해 순환 언어 모델과 인코더-디코더 아키텍처의 한계를 뛰어넘었습니다.

순환 모델은 일반적으로 입력 및 출력 시퀀스의 심볼 위치를 따라 계산을 분해합니다. 위치를 계산 시간 단계에 맞춰 조정하여 이전 은닉 상태 $$h_{t-1}$$과 위치 $$t$$에 대한 입력의 함수로 은닉 상태 $$h_t$$의 시퀀스를 생성합니다. 이러한 본질적으로 순차적인 특성은 훈련 예제 내에서 병렬화를 배제하며, 이는 메모리 제약으로 인해 예제 간의 배칭이 제한되므로 시퀀스 길이가 길어질수록 중요해집니다. 최근 연구에서는 인수분해 기법과 조건부 계산을 통해 계산 효율성을 크게 향상시키는 동시에 후자의 경우 모델 성능도 향상시켰습니다. 그러나 순차 계산의 근본적인 제약은 남아 있습니다.

어텐션 메커니즘은 다양한 작업에서 매력적인 시퀀스 모델링 및 변환 모델의 필수적인 부분이 되었으며, 입력 또는 출력 시퀀스에서의 거리에 관계없이 종속성을 모델링할 수 있습니다. 그러나 몇 가지 경우를 제외하고는 이러한 어텐션 메커니즘이 순환 네트워크와 함께 사용됩니다.

이 연구에서 우리는 순환을 피하고 대신 전적으로 어텐션 메커니즘에 의존하여 입력과 출력 간의 전역 종속성을 도출하는 모델 아키텍처인 트랜스포머를 제안합니다. 트랜스포머는 훨씬 더 많은 병렬화를 허용하며 8개의 P100 GPU에서 단 12시간만 훈련한 후 번역 품질에서 새로운 최첨단 기술에 도달할 수 있습니다.

## 2. 배경

순차적 계산을 줄이는 목표는 확장된 신경 GPU, ByteNet 및 ConvS2S의 기초를 형성하며, 이들 모두 컨볼루션 신경망을 기본 빌딩 블록으로 사용하여 모든 입력 및 출력 위치에 대한 은닉 표현을 병렬로 계산합니다. 이러한 모델에서 두 개의 임의 입력 또는 출력 위치의 신호를 관련시키는 데 필요한 작업 수는 위치 간의 거리에 따라 ConvS2S의 경우 선형적으로, ByteNet의 경우 로그적으로 증가합니다. 이로 인해 원거리 위치 간의 종속성을 학습하기가 더 어려워집니다. 트랜스포머에서는 가중 평균 위치로 인한 효과적인 해상도 감소의 대가로 일정한 수의 작업으로 축소되며, 이는 3.2절에서 설명하는 다중 헤드 어텐션으로 상쇄합니다.

셀프 어텐션(때로는 내부 어텐션이라고도 함)은 시퀀스의 표현을 계산하기 위해 단일 시퀀스의 다른 위치를 관련시키는 어텐션 메커니즘입니다. 셀프 어텐션은 읽기 이해, 추상적 요약, 텍스트 함의 및 작업 독립적인 문장 표현 학습을 포함한 다양한 작업에서 성공적으로 사용되었습니다.

엔드-투-엔드 메모리 네트워크는 시퀀스 정렬 순환 대신 순환 어텐션 메커니즘을 기반으로 하며 간단한 언어 질문 답변 및 언어 모델링 작업에서 잘 수행되는 것으로 나타났습니다.

그러나 우리가 아는 한, 트랜스포머는 시퀀스 정렬 RNN이나 컨볼루션을 사용하지 않고 입력과 출력의 표현을 계산하기 위해 전적으로 셀프 어텐션에 의존하는 최초의 변환 모델입니다. 다음 섹션에서는 트랜스포머에 대해 설명하고 셀프 어텐션을 동기 부여하며 다른 모델에 대한 이점을 논의합니다.

## 3. 모델 아키텍처

대부분의 경쟁력 있는 신경 시퀀스 변환 모델은 인코더-디코더 구조를 가지고 있습니다. 여기서 인코더는 기호 표현의 입력 시퀀스 $$(x_1, ..., x_n)$$을 연속적인 표현 $$z = (z_1, ..., z_n)$$의 시퀀스에 매핑합니다. $$z$$가 주어지면 디코더는 한 번에 하나의 요소를 기호의 출력 시퀀스 $$(y_1, ..., y_m)$$를 생성합니다. 각 단계에서 모델은 이전에 생성된 기호를 다음 기호를 생성할 때 추가 입력으로 사용하여 자기 회귀적입니다.

3.1 인코더 및 디코더 스택

**인코더:** 인코더는 $$N = 6$$개의 동일한 레이어 스택으로 구성됩니다. 각 레이어에는 두 개의 하위 레이어가 있습니다. 첫 번째는 다중 헤드 셀프 어텐션 메커니즘이고 두 번째는 간단한 위치별 완전 연결 피드포워드 네트워크입니다. 두 하위 레이어 각각에 잔여 연결을 사용하고 그 뒤에 레이어 정규화를 적용합니다. 즉, 각 하위 레이어의 출력은 $$LayerNorm(x + Sublayer(x))$$이며, 여기서 $$Sublayer(x)$$는 하위 레이어 자체에서 구현된 함수입니다. 이러한 잔여 연결을 용이하게 하기 위해 모델의 모든 하위 레이어와 임베딩 레이어는 차원이 $$d_{model} = 512$$인 출력을 생성합니다.

**디코더:** 디코더는 또한 $$N = 6$$개의 동일한 레이어 스택으로 구성됩니다. 각 인코더 레이어의 두 하위 레이어 외에 디코더는 인코더 스택의 출력에 대해 다중 헤드 어텐션을 수행하는 세 번째 하위 레이어를 삽입합니다. 인코더와 유사하게 각 하위 레이어 주위에 잔여 연결을 사용하고 그 뒤에 레이어 정규화를 적용합니다. 또한 디코더 스택의 셀프 어텐션 하위 레이어를 수정하여 위치가 후속 위치에 주의를 기울이는 것을 방지합니다. 이 마스킹은 출력 임베딩이 한 위치만큼 오프셋된다는 사실과 결합되어 위치 $$i$$에 대한 예측이 $$i$$보다 작은 위치의 알려진 출력에만 의존하도록 보장합니다.

### 3.2 어텐션

어텐션 함수는 쿼리와 키-값 쌍 집합을 출력에 매핑하는 것으로 설명할 수 있습니다. 여기서 쿼리, 키, 값 및 출력은 모두 벡터입니다. 출력은 값의 가중 합으로 계산되며, 각 값에 할당된 가중치는 해당 키와 쿼리의 호환성 함수에 의해 계산됩니다.

<figure><img src="broken-reference" alt="" width="563"><figcaption><p>Figure 2: (left) Scaled Dot-Product Attention. <br>(right) Multi-Head Attention consists of several attention layers running in parallel.</p></figcaption></figure>

#### 3.2.1 스케일드 닷-프로덕트 어텐션

우리는 우리의 특정 어텐션을 "스케일드 닷-프로덕트 어텐션"이라고 부릅니다. 입력은 차원 $$d_k$$의 쿼리 및 키와 차원 $$d_v$$의 값으로 구성됩니다. 우리는 쿼리와 모든 키의 내적을 계산하고 각각을 $$\sqrt{d_k}$$로 나눈 다음 소프트맥스 함수를 적용하여 값에 대한 가중치를 얻습니다.

실제로 우리는 쿼리 집합에 대한 어텐션 함수를 동시에 계산하여 행렬 $$Q$$로 묶습니다. 키와 값도 행렬 $$K$$와 $$V$$로 묶습니다. 출력 행렬은 다음과 같이 계산합니다.

$$Attention(Q, K, V) = softmax(\frac{Q K^T}{\sqrt{d_k}})V$$

가장 일반적으로 사용되는 두 가지 어텐션 함수는 추가 어텐션과 내적(곱셈) 어텐션입니다. 내적 어텐션은 스케일링 계수 $$\frac{1}{\sqrt{d_k}}$$를 제외하고 우리 알고리즘과 동일합니다. 추가 어텐션은 단일 은닉 레이어가 있는 피드포워드 네트워크를 사용하여 호환성 함수를 계산합니다. 이 두 가지는 이론적 복잡성은 비슷하지만, 고도로 최적화된 행렬 곱셈 코드를 사용하여 구현할 수 있기 때문에 실제로는 내적 어텐션이 훨씬 빠르고 공간 효율적입니다.

$$d_k$$의 작은 값의 경우 두 메커니즘이 유사하게 수행되지만, $$d_k$$의 큰 값의 경우 스케일링 없이 추가 어텐션이 내적 어텐션보다 성능이 뛰어납니다. 우리는 $$d_k$$의 큰 값의 경우 내적이 크기가 커져 소프트맥스 함수를 기울기가 극히 작은 영역으로 밀어 넣는다고 의심합니다. 이 효과를 상쇄하기 위해 내적을 $$\frac{1}{\sqrt{d_k}}$$로 스케일링합니다.

#### 3.2.2 다중 헤드 어텐션

$$d_{model}$$ 차원의 키, 값 및 쿼리를 사용하여 단일 어텐션 함수를 수행하는 대신, 쿼리, 키 및 값을 각각 다른 학습된 선형 투영으로 $$h$$번 선형적으로 투영하여 $$d_k$$, $$d_k$$ 및 $$d_v$$ 차원으로 만드는 것이 유익하다는 것을 발견했습니다. 이러한 각 투영된 버전의 쿼리, 키 및 값에 대해 병렬로 어텐션 함수를 수행하여 $$d_v$$ 차원 출력 값을 생성합니다. 이들은 연결되고 다시 한 번 투영되어 최종 값을 생성합니다.

다중 헤드 어텐션을 통해 모델은 다른 위치의 다른 표현 하위 공간의 정보에 공동으로 주의를 기울일 수 있습니다. 단일 어텐션 헤드를 사용하면 평균화로 인해 이것이 억제됩니다.

$$MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$$ $$where \ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$$

프로젝션은 매개변수 행렬 $$W_i^Q \in \mathbb{R}^{d_{model} \times d_k}$$, $$W_i^K \in \mathbb{R}^{d_{model} \times d_k}$$, $$W_i^V \in \mathbb{R}^{d_{model} \times d_v}$$ 및 $$W^O \in \mathbb{R}^{h d_v \times d_{model}}$$입니다.

이 연구에서는 $$h = 8$$개의 병렬 어텐션 레이어 또는 헤드를 사용합니다. 각각에 대해 $$d_k = d_v = d_{model}/h = 64$$를 사용합니다. 각 헤드의 차원이 줄어들기 때문에 총 계산 비용은 전체 차원을 가진 단일 헤드 어텐션과 유사합니다.

#### 3.2.3 우리 모델의 어텐션 적용

트랜스포머는 세 가지 다른 방식으로 다중 헤드 어텐션을 사용합니다.

* "인코더-디코더 어텐션" 레이어에서 쿼리는 이전 디코더 레이어에서 오고 메모리 키와 값은 인코더의 출력에서 옵니다. 이를 통해 디코더의 모든 위치가 입력 시퀀스의 모든 위치에 주의를 기울일 수 있습니다.
* 인코더에는 셀프 어텐션 레이어가 포함되어 있습니다. 셀프 어텐션 레이어에서 모든 키, 값 및 쿼리는 동일한 위치, 이 경우 인코더의 이전 레이어 출력에서 옵니다.
* 유사하게, 디코더의 셀프 어텐션 레이어는 디코더의 각 위치가 해당 위치까지 포함하여 디코더의 모든 위치에 주의를 기울일 수 있도록 합니다. 자동 회귀 속성을 보존하기 위해 디코더에서 왼쪽으로의 정보 흐름을 방지해야 합니다. 불법적인 연결에 해당하는 소프트맥스 입력의 모든 값을 마스킹(–$$\infty$$로 설정)하여 스케일드 닷-프로덕트 어텐션 내부에서 이를 구현합니다.

### 3.3 위치별 피드포워드 네트워크

어텐션 하위 레이어 외에도 인코더와 디코더의 각 레이어에는 완전 연결 피드포워드 네트워크가 포함되어 있으며, 이는 각 위치에 개별적으로 동일하게 적용됩니다. 이것은 두 개의 선형 변환과 그 사이에 ReLU 활성화로 구성됩니다.

$$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$$

선형 변환은 다른 위치에서 동일하지만 레이어마다 다른 매개변수를 사용합니다. 이를 설명하는 또 다른 방법은 커널 크기가 1인 두 개의 컨볼루션입니다. 입력 및 출력의 차원은 $$d_{model} = 512$$이고 내부 레이어의 차원은 $$d_{ff} = 2048$$입니다.

### 3.4 임베딩 및 소프트맥스

다른 시퀀스 변환 모델과 유사하게, 학습된 임베딩을 사용하여 입력 토큰과 출력 토큰을 차원 $$d_{model}$$의 벡터로 변환합니다. 또한 일반적인 학습된 선형 변환 및 소프트맥스 함수를 사용하여 디코더 출력을 예측된 다음 토큰 확률로 변환합니다. 우리 모델에서는 두 임베딩 레이어와 사전 소프트맥스 선형 변환 간에 동일한 가중치 행렬을 공유합니다. 임베딩 레이어에서는 이러한 가중치에 $$\sqrt{d_{model}}$$을 곱합니다.

### 3.5 위치 인코딩

우리 모델에는 순환이나 컨볼루션이 없기 때문에 시퀀스의 순서를 활용하려면 시퀀스의 토큰에 대한 상대적 또는 절대적 위치에 대한 정보를 주입해야 합니다. 이를 위해 인코더 및 디코더 스택 하단의 입력 임베딩에 "위치 인코딩"을 추가합니다. 위치 인코딩은 임베딩과 동일한 차원 $$d_{model}$$을 가지므로 둘을 합산할 수 있습니다.

이 연구에서는 다양한 주파수의 사인 및 코사인 함수를 사용합니다.

$$PE_{(pos, 2i)} = sin(pos / 10000^{2i/d_{model}})$$ $$PE_{(pos, 2i+1)} = cos(pos / 10000^{2i/d_{model}})$$

여기서 $$pos$$는 위치이고 $$i$$는 차원입니다. 즉, 위치 인코딩의 각 차원은 사인파에 해당합니다. 파장은 $$2\pi$$에서 $$10000 \cdot 2\pi$$까지 기하급수적으로 증가합니다. 고정된 오프셋 $$k$$에 대해 $$PE_{pos+k}$$가 $$PE_{pos}$$의 선형 함수로 표현될 수 있기 때문에 모델이 상대적 위치에 의해 쉽게 주의를 기울이는 법을 배울 수 있을 것이라고 가정했기 때문에 이 함수를 선택했습니다.

## 4. 셀프 어텐션이 필요한 이유

이 섹션에서는 셀프 어텐션 레이어의 다양한 측면을 기호 표현의 가변 길이 시퀀스 $$(x_1, ..., x_n)$$를 동일한 길이의 다른 시퀀스 $$(z_1, ..., z_n)$$에 매핑하는 데 일반적으로 사용되는 순환 및 컨볼루션 레이어와 비교합니다. 여기서 $$x_i, z_i \in \mathbb{R}^d$$이며, 일반적인 시퀀스 변환 인코더 또는 디코더의 은닉 레이어와 같습니다. 셀프 어텐션을 사용하는 동기를 부여하기 위해 세 가지 희망 사항을 고려합니다.

하나는 레이어당 총 계산 복잡성입니다. 다른 하나는 최소 순차 작업 수로 측정되는 병렬화할 수 있는 계산량입니다.

세 번째는 네트워크에서 장거리 종속성 간의 경로 길이입니다. 많은 시퀀스 변환 작업에서 장거리 종속성을 학습하는 것은 핵심적인 과제입니다. 이러한 종속성을 학습하는 능력에 영향을 미치는 한 가지 핵심 요소는 신호가 네트워크를 순방향 및 역방향으로 통과해야 하는 경로의 길이입니다. 입력 및 출력 시퀀스의 모든 위치 조합 간의 경로가 짧을수록 장거리 종속성을 배우기가 더 쉽습니다. 따라서 우리는 다른 레이어 유형으로 구성된 네트워크에서 두 개의 입력 및 출력 위치 간의 최대 경로 길이도 비교합니다.

표 1에서 언급했듯이 셀프 어텐션 레이어는 모든 위치를 일정한 수의 순차적으로 실행되는 작업으로 연결하는 반면, 순환 레이어는 $$O(n)$$ 순차 작업이 필요합니다. 계산 복잡성 측면에서 시퀀스 길이 $$n$$이 표현 차원 $$d$$보다 작을 때 셀프 어텐션 레이어는 순환 레이어보다 빠르며, 이는 단어 조각 및 바이트 쌍 표현과 같이 기계 번역에서 최첨단 모델에서 사용되는 문장 표현의 경우 가장 흔합니다. 매우 긴 시퀀스를 포함하는 작업의 계산 성능을 향상시키기 위해 셀프 어텐션은 각 출력 위치를 중심으로 입력 시퀀스에서 크기 $$r$$의 이웃만 고려하도록 제한될 수 있습니다. 이렇게 하면 최대 경로 길이가 $$O(n/r)$$로 증가합니다. 우리는 향후 연구에서 이 접근 방식을 더 자세히 조사할 계획입니다.

커널 너비가 $$k < n$$인 단일 컨볼루션 레이어는 모든 입력 및 출력 위치 쌍을 연결하지 않습니다. 그렇게 하려면 연속적인 커널의 경우 $$O(n/k)$$ 컨볼루션 레이어 스택이 필요하거나 확장된 컨볼루션의 경우 $$O(log_k(n))$$이 필요하여 네트워크의 두 위치 간의 가장 긴 경로 길이가 증가합니다. 컨볼루션 레이어는 일반적으로 순환 레이어보다 $$k$$배 더 비쌉니다. 그러나 분리 가능한 컨볼루션은 복잡성을 $$O(k \cdot n \cdot d + n \cdot d^2)$$로 크게 감소시킵니다. 그러나 $$k = n$$인 경우에도 분리 가능한 컨볼루션의 복잡성은 셀프 어텐션 레이어와 위치별 피드포워드 레이어의 조합과 동일하며, 이는 우리 모델에서 취하는 접근 방식입니다.

부수적인 이점으로 셀프 어텐션은 더 해석 가능한 모델을 생성할 수 있습니다. 우리는 모델의 어텐션 분포를 검사하고 부록에서 예제를 제시하고 논의합니다. 개별 어텐션 헤드는 다른 작업을 수행하는 법을 명확하게 배울 뿐만 아니라 많은 경우 문장의 구문 및 의미 구조와 관련된 동작을 나타냅니다.

## 5. 훈련

이 섹션에서는 모델의 훈련 체제를 설명합니다.

### 5.1 훈련 데이터 및 배치

우리는 약 450만 개의 문장 쌍으로 구성된 표준 WMT 2014 영어-독일어 데이터 세트에서 훈련했습니다. 문장은 약 37,000개의 토큰으로 구성된 공유 소스-타겟 어휘를 가진 바이트 쌍 인코딩을 사용하여 인코딩되었습니다. 영어-프랑스어의 경우, 3,600만 개의 문장으로 구성된 훨씬 더 큰 WMT 2014 영어-프랑스어 데이터 세트를 사용하고 토큰을 32,000개의 단어 조각 어휘로 분할했습니다. 문장 쌍은 대략적인 시퀀스 길이에 따라 함께 배치되었습니다. 각 훈련 배치에는 약 25,000개의 소스 토큰과 25,000개의 타겟 토큰을 포함하는 문장 쌍 집합이 포함되었습니다.

### 5.2 하드웨어 및 일정

우리는 8개의 NVIDIA P100 GPU가 있는 한 대의 머신에서 모델을 훈련했습니다. 논문 전체에서 설명된 하이퍼파라미터를 사용하는 기본 모델의 경우 각 훈련 단계에 약 0.4초가 걸렸습니다. 기본 모델을 총 100,000단계 또는 12시간 동안 훈련했습니다. 대형 모델의 경우 단계 시간은 1.0초였습니다. 대형 모델은 300,000단계(3.5일) 동안 훈련되었습니다.

### 5.3 옵티마이저

Adam 옵티마이저를 $$\beta_1 = 0.9$$, $$\beta_2 = 0.98$$ 및 $$\varepsilon = 10^{-9}$$와 함께 사용했습니다. 훈련 과정에서 학습률을 다음 공식에 따라 변경했습니다.

$$lrate = d_{model}^{-0.5} \cdot min(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5})$$

이것은 첫 번째 $$warmup\_steps$$ 훈련 단계 동안 학습률을 선형적으로 증가시킨 다음 단계 수의 역제곱근에 비례하여 감소시키는 것에 해당합니다. $$warmup\_steps = 4000$$을 사용했습니다.

### 5.4 정규화

훈련 중에 세 가지 유형의 정규화를 사용합니다.

**잔여 드롭아웃** 각 하위 레이어의 출력에 드롭아웃을 적용한 다음 하위 레이어 입력에 추가하고 정규화합니다. 또한 인코더 및 디코더 스택 모두에서 임베딩과 위치 인코딩의 합계에 드롭아웃을 적용합니다. 기본 모델의 경우 $$P_{drop} = 0.1$$의 비율을 사용합니다.

**레이블 스무딩** 훈련 중에 값 $$\varepsilon_{ls} = 0.1$$의 레이블 스무딩을 사용했습니다. 이것은 모델이 더 불확실하게 학습함에 따라 혼란도를 손상시키지만 정확도와 BLEU 점수를 향상시킵니다.

## 6. 결과

### 6.1 기계 번역

WMT 2014 영어-독일어 번역 작업에서 대형 트랜스포머 모델(표 2의 트랜스포머(대형))은 이전에 보고된 최고의 모델(앙상블 포함)보다 2.0 BLEU 이상 성능이 뛰어나 28.4의 새로운 최첨단 BLEU 점수를 수립했습니다. 이 모델의 구성은 표 3의 맨 아래 줄에 나열되어 있습니다. 훈련은 8개의 P100 GPU에서 3.5일이 걸렸습니다. 기본 모델조차도 이전에 게시된 모든 모델 및 앙상블을 능가하며 경쟁 모델의 훈련 비용의 일부만으로도 가능합니다.

WMT 2014 영어-프랑스어 번역 작업에서 우리의 대형 모델은 41.0의 BLEU 점수를 달성하여 이전에 게시된 모든 단일 모델보다 성능이 뛰어나며 이전 최첨단 모델의 훈련 비용의 1/4 미만입니다. 영어-프랑스어용으로 훈련된 트랜스포머(대형) 모델은 0.3 대신 드롭아웃 비율 $$P_{drop} = 0.1$$을 사용했습니다.

기본 모델의 경우 10분 간격으로 작성된 마지막 5개의 체크포인트를 평균하여 얻은 단일 모델을 사용했습니다. 대형 모델의 경우 마지막 20개의 체크포인트를 평균했습니다. 빔 크기가 4이고 길이 페널티가 $$\alpha = 0.6$$인 빔 검색을 사용했습니다. 이러한 하이퍼파라미터는 개발 세트에서 실험한 후 선택되었습니다. 추론 중 최대 출력 길이를 입력 길이 + 50으로 설정했지만 가능하면 조기에 종료합니다.

**표 2: 트랜스포머는 이전 최첨단 모델보다 더 나은 BLEU 점수를 달성하면서도 훈련 비용은 일부에 불과합니다 (영어-독일어 및 영어-프랑스어 newstest2014 테스트).**

| 모델                    | BLEU (EN-DE) | BLEU (EN-FR) | 훈련 비용 (FLOPs) (EN-DE) | 훈련 비용 (FLOPs) (EN-FR) |
| --------------------- | ------------ | ------------ | --------------------- | --------------------- |
| ByteNet               | 23.75        |              |                       |                       |
| Deep-Att + PosUnk     |              | 39.2         |                       | $$1.0 \cdot 10^{20}$$ |
| GNMT + RL             | 24.6         | 39.92        | $$2.3 \cdot 10^{19}$$ | $$1.4 \cdot 10^{20}$$ |
| ConvS2S               | 25.16        | 40.46        | $$9.6 \cdot 10^{18}$$ | $$1.5 \cdot 10^{20}$$ |
| MoE                   | 26.03        | 40.56        | $$2.0 \cdot 10^{19}$$ | $$1.2 \cdot 10^{20}$$ |
| Deep-Att + PosUnk 앙상블 |              | 40.4         |                       | $$8.0 \cdot 10^{20}$$ |
| GNMT + RL 앙상블         | 26.30        | 41.16        | $$1.8 \cdot 10^{20}$$ | $$1.1 \cdot 10^{21}$$ |
| ConvS2S 앙상블           | 26.36        | 41.29        | $$7.7 \cdot 10^{19}$$ | $$1.2 \cdot 10^{21}$$ |
| **트랜스포머 (기본 모델)**     | **27.3**     | **38.1**     | $$3.3 \cdot 10^{18}$$ |                       |
| **트랜스포머 (대형)**        | **28.4**     | **41.8**     | $$2.3 \cdot 10^{19}$$ |                       |

### 6.2 모델 변형

트랜스포머의 다른 구성 요소의 중요성을 평가하기 위해 기본 모델을 다양한 방식으로 변경하여 개발 세트인 newstest2013에서 영어-독일어 번역 성능 변화를 측정했습니다. 이전 섹션에서 설명한 대로 빔 검색을 사용했지만 체크포인트 평균화는 사용하지 않았습니다. 이 결과를 표 3에 제시합니다.

**표 3: 트랜스포머 아키텍처의 변형. 나열되지 않은 값은 기본 모델과 동일합니다. 모든 메트릭은 영어-독일어 번역 개발 세트, newstest2013에 대한 것입니다. 나열된 혼란도는 바이트 쌍 인코딩에 따른 단어 조각당 혼란도이며 단어당 혼란도와 비교해서는 안 됩니다.**

|      |                               N                               | $$d_{model}$$ | $$d_{ff}$$ |  h  | $$d_k$$ | $$d_v$$ | $$P_{drop}$$ | $$\varepsilon_{ls}$$ | train steps | PPL (dev) | BLEU (dev) | params $$\times 10^6$$ |
| ---- | :-----------------------------------------------------------: | :-----------: | :--------: | :-: | :-----: | :-----: | :----------: | :------------------: | :---------: | :-------: | :--------: | :--------------------: |
| base |                               6                               |      512      |    2048    |  8  |    64   |    64   |      0.1     |          0.1         |     100K    |    4.92   |    25.8    |           65           |
| (A)  |                                                               |               |            |  1  |   512   |   512   |              |                      |             |    5.29   |    24.9    |                        |
|      |                                                               |               |            |  4  |   128   |   128   |              |                      |             |    5.00   |    25.5    |                        |
|      |                                                               |               |            |  16 |    32   |    32   |              |                      |             |    4.91   |    25.8    |                        |
|      |                                                               |               |            |  32 |    16   |    16   |              |                      |             |    5.01   |    25.4    |                        |
| (B)  |                                                               |               |            |     |    16   |         |              |                      |             |    5.16   |    25.1    |           58           |
|      |                                                               |               |            |     |    32   |         |              |                      |             |    5.01   |    25.4    |           60           |
| (C)  |                               2                               |               |            |     |         |         |              |                      |             |    6.11   |    23.7    |           36           |
|      |                               4                               |               |            |     |         |         |              |                      |             |    5.19   |    25.3    |           50           |
|      |                               8                               |               |            |     |         |         |              |                      |             |    4.88   |    25.5    |           80           |
|      |                                                               |      256      |            |     |    32   |    32   |              |                      |             |    5.75   |    24.5    |           28           |
|      |                                                               |      1024     |            |     |   128   |   128   |              |                      |             |    4.66   |    26.0    |           168          |
|      |                                                               |               |    1024    |     |         |         |              |                      |             |    5.12   |    25.4    |           53           |
|      |                                                               |               |    4096    |     |         |         |              |                      |             |    4.75   |    26.2    |           90           |
| (D)  |                                                               |               |            |     |         |         |      0.0     |                      |             |    5.77   |    24.6    |                        |
|      |                                                               |               |            |     |         |         |      0.2     |                      |             |    4.95   |    25.5    |                        |
|      |                                                               |               |            |     |         |         |              |          0.0         |             |    4.67   |    25.3    |                        |
|      |                                                               |               |            |     |         |         |              |          0.2         |             |    5.47   |    25.7    |                        |
| (E)  | \multicolumn{6}{c}{positional embedding instead of sinusoids} |               |            |     |   4.92  |   25.7  |              |                      |             |           |            |                        |
| big  |                               6                               |      1024     |    4096    |  16 |         |         |      0.3     |                      |     300K    |    4.33   |    26.4    |           213          |

### 6.3 영어 구문 분석

트랜스포머가 다른 작업으로 일반화될 수 있는지 평가하기 위해 영어 구문 분석에 대한 실험을 수행했습니다. 이 작업은 특정한 과제를 제시합니다. 출력은 강력한 구조적 제약을 받으며 입력보다 훨씬 깁니다. 또한 RNN 시퀀스-투-시퀀스 모델은 소규모 데이터 체제에서 최첨단 결과를 얻지 못했습니다.

**표 4: 트랜스포머는 영어 구문 분석에도 잘 일반화됩니다 (결과는 WSJ의 섹션 23에 대한 것입니다).**

| 파서                             | 훈련                           | WSJ 23 F1 |
| ------------------------------ | ---------------------------- | --------- |
| Vinyals & Kaiser et al. (2014) | WSJ only, discriminative     | 88.3      |
| Petrov et al. (2006)           | WSJ only, discriminative     | 90.4      |
| Zhu et al. (2013)              | WSJ only, discriminative     | 90.4      |
| Dyer et al. (2016)             | WSJ only, discriminative     | 91.7      |
| **트랜스포머 (4 레이어)**              | **WSJ only, discriminative** | **91.3**  |
| Zhu et al. (2013)              | semi-supervised              | 91.3      |
| Huang & Harper (2009)          | semi-supervised              | 91.3      |
| McClosky et al. (2006)         | semi-supervised              | 92.1      |
| Vinyals & Kaiser et al. (2014) | semi-supervised              | 92.1      |
| **트랜스포머 (4 레이어)**              | **semi-supervised**          | **92.7**  |
| Luong et al. (2015)            | multi-task                   | 93.0      |
| Dyer et al. (2016)             | generative                   | 93.3      |

## 7. 결론

이 연구에서는 인코더-디코더 아키텍처에서 가장 일반적으로 사용되는 순환 레이어를 다중 헤드 셀프 어텐션으로 대체하여 전적으로 어텐션을 기반으로 하는 최초의 시퀀스 변환 모델인 트랜스포머를 제시했습니다.

번역 작업의 경우 트랜스포머는 순환 또는 컨볼루션 레이어를 기반으로 하는 아키텍처보다 훨씬 빠르게 훈련될 수 있습니다. WMT 2014 영어-독일어 및 WMT 2014 영어-프랑스어 번역 작업 모두에서 우리는 새로운 최첨단 기술을 달성했습니다. 이전 작업에서 우리 최고의 모델은 이전에 보고된 모든 앙상블보다 성능이 뛰어납니다.

우리는 어텐션 기반 모델의 미래에 대해 기대하며 다른 작업에 적용할 계획입니다. 우리는 트랜스포머를 텍스트 이외의 입력 및 출력 양식을 포함하는 문제로 확장하고 이미지, 오디오 및 비디오와 같은 대규모 입력 및 출력을 효율적으로 처리하기 위해 로컬, 제한된 어텐션 메커니즘을 조사할 계획입니다. 생성을 덜 순차적으로 만드는 것이 우리의 또 다른 연구 목표입니다.

모델을 훈련하고 평가하는 데 사용한 코드는 https://github.com/tensorflow/tensor2tensor에서 사용할 수 있습니다.

**감사의 말** 우리는 Nal Kalchbrenner와 Stephan Gouws의 유익한 의견, 수정 및 영감에 감사합니다.
